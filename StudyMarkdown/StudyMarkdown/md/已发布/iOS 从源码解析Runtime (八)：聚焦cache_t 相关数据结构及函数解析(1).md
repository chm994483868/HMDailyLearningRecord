# iOS ä»æºç è§£æRuntime (å…«)ï¼šèšç„¦cache_t ç›¸å…³æ•°æ®ç»“æ„åŠå‡½æ•°è§£æ(1)

> &emsp;å‰é¢è¿ç»­å‡ ç¯‡æˆ‘ä»¬å·²ç»è¯¦ç»†åˆ†æäº† `objc_object` çš„ç›¸å…³çš„æ‰€æœ‰æºç ï¼Œæ¥ä¸‹æ¥å‡ ç¯‡åˆ™å¼€å§‹åˆ†æå®šä¹‰äº `objc-runtime-new.h` ä¸­çš„ `objc_class`ï¼Œæœ¬ç¯‡å…ˆä» `struct objc_class : objc_object` çš„ `cache_t cache` å¼€å§‹ï¼Œ`cache_t` ä¸»è¦å®ç°æ–¹æ³•ç¼“å­˜ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¿«çš„æ‰¾åˆ°æ–¹æ³•åœ°å€è¿›è¡Œè°ƒç”¨ã€‚
  çºµè§ˆ `objc-runtime-new.h` æ–‡ä»¶çœŸçš„è¶…é•¿ï¼Œé‚£æˆ‘ä»¬å°±åˆ†å—æ¥å­¦ä¹ ï¼Œä¸€èµ· â›½ï¸â›½ï¸ å§ï¼

```c++
struct objc_class : objc_object {
// Class ISA; // objc_class ç»§æ‰¿è‡ª objc_objectï¼Œæ‰€ä»¥å…¶ç¬¬ä¸€ä¸ªæˆå‘˜å˜é‡å…¶å®æ˜¯ isa_t isa 
Class superclass; // çˆ¶ç±»æŒ‡é’ˆ
cache_t cache; // formerly cache pointer and vtable ä»¥å‰ç¼“å­˜æŒ‡é’ˆå’Œè™šå‡½æ•°è¡¨
...
};
```
> &emsp;`typedef uintptr_t SEL;` åœ¨ `objc-runtime-new.h` çš„ `198` è¡Œï¼Œçœ‹åˆ° `SEL`ã€‚ 

&emsp;`cache` æ˜¯ `objc_class` çš„ç¬¬ä¸‰ä¸ªæˆå‘˜å˜é‡ï¼Œç±»å‹æ˜¯ `cache_t`ã€‚ä»æ•°æ®ç»“æ„è§’åº¦åŠä½¿ç”¨æ–¹æ³•æ¥çœ‹ `cache_t` çš„è¯ï¼Œå®ƒæ˜¯ä¸€ä¸ª `SEL`  ä½œä¸º `Key` ï¼Œ`SEL + IMP(bucket_t)` ä½œä¸º `Value` çš„æ•£åˆ—è¡¨ã€‚ä¸ºäº†å¯¹æ–¹æ³•ç¼“å­˜å…ˆæœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ï¼Œæˆ‘ä»¬é¦–å…ˆè§£è¯»ä¸€ä¸‹ `objc-cache.mm` æ–‡ä»¶å¼€å¤´çš„ä¸€å¤§æ®µæ³¨é‡Šå†…å®¹ã€‚
```c++
/*
objc-cache.m

1. Method cache management æ–¹æ³•ç¼“å­˜ç®¡ç†
2. Cache flushing ç¼“å­˜åˆ·æ–°
3. Cache garbage collection ç¼“å­˜åƒåœ¾å›æ”¶
4. Cache instrumentation ç¼“å­˜æ£€æµ‹
5. Dedicated allocator for large caches å¤§å‹ç¼“å­˜çš„ä¸“ç”¨åˆ†é…å™¨ (?)

/*
Method cache locking (GrP 2001-1-14)
For speed, objc_msgSend does not acquire any locks when it reads method caches. 
ä¸ºäº†æé«˜é€Ÿåº¦ï¼Œobjc_msgSend åœ¨è¯»å–æ–¹æ³•ç¼“å­˜æ—¶ä¸è·å–ä»»ä½•é”ã€‚

Instead, all cache changes are performed
so that any objc_msgSend running concurrently with the cache mutator 
will not crash or hang or get an incorrect result from the cache. 
ç›¸åï¼Œå°†æ‰§è¡Œæ‰€æœ‰ç¼“å­˜æ›´æ”¹ï¼Œä»¥ä¾¿ä¸ç¼“å­˜ mutator å¹¶å‘è¿è¡Œçš„ä»»ä½• objc_msgSend éƒ½ä¸ä¼šå´©æºƒæˆ–æŒ‚èµ·ï¼Œ
æˆ–è€…ä»ç¼“å­˜ä¸­è·å¾—ä¸æ­£ç¡®çš„ç»“æœã€‚ï¼ˆä»¥ std::atomic å®Œæˆæ‰€æœ‰çš„åŸå­æ“ä½œï¼‰

When cache memory becomes unused (e.g. the old cache after cache expansion), 
it is not immediately freed, because a concurrent objc_msgSend could still be using it. 
å½“ç¼“å­˜æœªä½¿ç”¨æ—¶ï¼Œï¼ˆä¾‹å¦‚ï¼šç¼“å­˜æ‰©å±•åçš„æ—§ç¼“å­˜ï¼‰ï¼Œå®ƒä¸ä¼šç«‹å³é‡Šæ”¾ï¼Œå› ä¸ºå¹¶å‘çš„ objc_msgSend å¯èƒ½ä»åœ¨ä½¿ç”¨å®ƒã€‚

Instead, the memory is disconnected from the data structures and placed on a garbage list. 
ç›¸åï¼Œå†…å­˜ä¸æ•°æ®ç»“æ„æ–­å¼€è¿æ¥ï¼Œå¹¶æ”¾åœ¨åƒåœ¾åˆ—è¡¨ä¸­ã€‚

The memory is now only accessible to instances of objc_msgSend that were
running when the memory was disconnected; 
å†…å­˜ç°åœ¨åªèƒ½è®¿é—®æ–­å¼€å†…å­˜æ—¶è¿è¡Œçš„ objc_msgSend å®ä¾‹ï¼›

any further calls to objc_msgSend will not see the garbage memory because
the other data structures don't point to it anymore.
å¯¹ objc_msgSend çš„ä»»ä½•è¿›ä¸€æ­¥è°ƒç”¨éƒ½ä¸ä¼šçœ‹åˆ°åƒåœ¾å†…å­˜ï¼Œå› ä¸ºå…¶ä»–æ•°æ®ç»“æ„ä¸å†æŒ‡å‘å®ƒã€‚

The collecting_in_critical function checks the PC of all threads and returns FALSE when
all threads are found to be outside objc_msgSend. 
collecting_in_critical å‡½æ•°æ£€æŸ¥æ‰€æœ‰çº¿ç¨‹çš„PCï¼Œå½“å‘ç°æ‰€æœ‰çº¿ç¨‹éƒ½åœ¨ objc_msgSend ä¹‹å¤–æ—¶è¿”å› FALSEã€‚

This means any call to objc_msgSend that could have had access to the garbage has
finished or moved past the cache lookup stage, so it is safe to free the memory. 
è¿™æ„å‘³ç€å¯ä»¥è®¿é—®åƒåœ¾çš„å¯¹ objc_msgSend çš„ä»»ä½•è°ƒç”¨éƒ½å·²å®Œæˆæˆ–ç§»åŠ¨åˆ°ç¼“å­˜æŸ¥æ‰¾é˜¶æ®µï¼Œå› æ­¤å¯ä»¥å®‰å…¨åœ°é‡Šæ”¾å†…å­˜ã€‚

All functions that modify cache data or structures must acquire the cacheUpdateLock
to prevent interference from concurrent modifications. 
æ‰€æœ‰ä¿®æ”¹ç¼“å­˜æ•°æ®æˆ–ç»“æ„çš„å‡½æ•°éƒ½å¿…é¡»è·å– cacheUpdateLockï¼Œä»¥é˜²æ­¢å¹¶å‘ä¿®æ”¹çš„å¹²æ‰°ã€‚

The function that frees cache garbage must acquire the cacheUpdateLock and use
collecting_in_critical() to flush out cache readers.
é‡Šæ”¾ç¼“å­˜åƒåœ¾çš„å‡½æ•°å¿…é¡»è·å– cacheUpdateLockï¼Œå¹¶ä½¿ç”¨ collecting_in_critical() æ¸…é™¤ç¼“å­˜è¯»å–å™¨

The cacheUpdateLock is also used to protect the custom allocator used for large method cache blocks.
Cache readers (PC-checked by collecting_in_critical())
cacheUpdateLock è¿˜ç”¨äºä¿æŠ¤ç”¨äºå¤§å‹æ–¹æ³•ç¼“å­˜å—çš„è‡ªå®šä¹‰åˆ†é…å™¨ã€‚ç¼“å­˜è¯»å–å™¨ï¼ˆç”± collecting_in_critical() è¿›è¡Œ PC æ£€æŸ¥ï¼‰

objc_msgSend
cache_getImp

Cache writers (hold cacheUpdateLock while reading or writing; not PC-checked)
(è¯»å–æˆ–å†™å…¥æ—¶è·å– cacheUpdateLockï¼Œä¸ä½¿ç”¨ PC-checked)

cache_fill         (acquires lock)(è·å–é”)
cache_expand       (only called from cache_fill)(ä»…ä» cache_fill è°ƒç”¨)
cache_create       (only called from cache_expand)(ä»…ä» cache_expand è°ƒç”¨)
bcopy               (only called from instrumented cache_expand)(ä»…ä»å·²æ£€æµ‹çš„ cache_expand è°ƒç”¨)
flush_caches        (acquires lock)(è·å–é”)
cache_flush        (only called from cache_fill and flush_caches)(ä»…ä» cache_fill å’Œ flush_caches è°ƒç”¨)
cache_collect_free (only called from cache_expand and cache_flush)(ä»…ä» cache_expand å’Œ cache_flush è°ƒç”¨)

UNPROTECTED cache readers (NOT thread-safe; used for debug info only)(ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼›ä»…ç”¨äºè°ƒè¯•ä¿¡æ¯)
cache_print cache æ‰“å°
_class_printMethodCaches
_class_printDuplicateCacheEntries
_class_printMethodCacheStatistics
*/
```
&emsp;åˆ°è¿™é‡Œå°±çœ‹å®Œæ³¨é‡Šäº†ï¼Œæœ‰ç‚¹æ‡µï¼Œä¸‹é¢è¿˜æ˜¯æŠŠæºç ä¸€è¡Œä¸€è¡Œçœ‹å®Œï¼Œç„¶åå†å›é¡¾ä¸Šé¢çš„å†…å®¹åˆ°åº•æŒ‡çš„æ˜¯ä»€ä¹ˆã€‚

## CACHE_IMP_ENCODING/CACHE_MASK_STORAGE
&emsp;åœ¨è¿›å…¥ `cache_t/bucket_t` å†…å®¹ä¹‹å‰ï¼Œé¦–å…ˆçœ‹ä¸¤ä¸ªå®å®šä¹‰ï¼Œ`CACHE_IMP_ENCODING` è¡¨ç¤ºåœ¨ `bucket_t` ä¸­ `IMP` çš„å­˜å‚¨æ–¹å¼ï¼Œ`CACHE_MASK_STORAGE` è¡¨ç¤º `cache_t` ä¸­æ©ç çš„ä½ç½®ã€‚`struct bucket_t` å’Œ `struct cache_t` é‡Œé¢çš„ä¸åŒå®ç°éƒ¨åˆ†æ­£æ˜¯æ ¹æ®è¿™ä¸¤ä¸ªå®æ¥åˆ¤æ–­çš„ã€‚æˆ‘ä»¬æœ€å…³æ³¨çš„ `x86_64(mac)` å’Œ `arm64(iphone)` ä¸¤ä¸ªå¹³å°ä¸‹ `bucket_t` ä¸­ `IMP` éƒ½æ˜¯ä»¥ `ISA` ä¸ `IMP` å¼‚æˆ–çš„å€¼å­˜å‚¨ã€‚è€Œæ©ç ä½ç½®çš„è¯ `x86_64` ä¸‹æ˜¯ `CACHE_MASK_STORAGE_OUTLINED` æ²¡æœ‰æ©ç ï¼Œ`buckets` æ•£åˆ—æ•°ç»„å’Œ `_mask` ä»¥ä¸¤ä¸ªæˆå‘˜å˜é‡åˆ†åˆ«è¡¨ç¤ºã€‚åœ¨ `arm64` ä¸‹åˆ™æ˜¯ `CACHE_MASK_STORAGE_HIGH_16` é«˜ `16` ä½ä¸ºæ©ç ï¼Œæ•£åˆ—æ•°ç»„å’Œ `mask` å…±åŒä¿å­˜åœ¨ `_maskAndBuckets` ä¸­ã€‚ 
```c++
// ä¸‰ç§æ–¹æ³•ç¼“å­˜å­˜å‚¨ IMP çš„æ–¹å¼ï¼šï¼ˆbucket_t ä¸­ _imp æˆå‘˜å˜é‡å­˜å‚¨ IMP çš„æ–¹å¼ï¼‰
// Determine how the method cache stores IMPs. 
// ç¡®å®šæ–¹æ³•ç¼“å­˜å¦‚ä½•å­˜å‚¨ IMPs. (IMP æ˜¯å‡½æ•°å®ç°çš„æŒ‡é’ˆï¼Œä¿å­˜äº†å‡½æ•°å®ç°çš„åœ°å€ï¼Œæ ¹æ® IMP å¯ä»¥ç›´æ¥æ‰§è¡Œå‡½æ•°...)

// Method cache contains raw IMP. æ–¹æ³•ç¼“å­˜åŒ…å«åŸå§‹çš„ IMPï¼ˆbucket_t ä¸­ _imp ä¸º IMPï¼‰
#define CACHE_IMP_ENCODING_NONE 1 

// Method cache contains ISA ^ IMP. æ–¹æ³•ç¼“å­˜åŒ…å« ISA ä¸ IMP çš„å¼‚æˆ–ï¼ˆbucket_t ä¸­ _imp æ˜¯ IMP ^ ISAï¼‰
#define CACHE_IMP_ENCODING_ISA_XOR 2 

// Method cache contains ptrauth'd IMP. 
// æ–¹æ³•ç¼“å­˜åŒ…å«æŒ‡é’ˆéªŒè¯çš„ IMP (bucket_t ä¸­ _imp æ˜¯ ptrauth_auth_and_resign å‡½æ•°è®¡ç®—çš„å€¼)
#define CACHE_IMP_ENCODING_PTRAUTH 3 

// ä¸Šè¿°ä¸‰ç§æ–¹å¼çš„å„åœ¨ä»€ä¹ˆå¹³å°ä½¿ç”¨ï¼š

#if __PTRAUTH_INTRINSICS__ // æœªæ‰¾åˆ°è¯¥å®å®šä¹‰çš„å€¼

// Always use ptrauth when it's supported. å½“å¹³å°æ”¯æŒ __PTRAUTH_INTRINSICS__ æ—¶æ€»æ˜¯ä½¿ç”¨æŒ‡é’ˆéªŒè¯
// æ­¤æ—¶ CACHE_IMP_ENCODING ä¸º CACHE_IMP_ENCODING_PTRAUTH

#define CACHE_IMP_ENCODING CACHE_IMP_ENCODING_PTRAUTH

#elif defined(__arm__)

// 32-bit ARM uses no encoding. 32ä½ ARM ä¸‹ä¸è¿›è¡Œç¼–ç ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ IMP(watchOS ä¸‹) 
#define CACHE_IMP_ENCODING CACHE_IMP_ENCODING_NONE

#else

// Everything else uses ISA ^ IMP. å…¶å®ƒæƒ…å†µä¸‹åœ¨æ–¹æ³•ç¼“å­˜ä¸­å­˜å‚¨ ISA ä¸ IMP å¼‚æˆ–çš„å€¼
#define CACHE_IMP_ENCODING CACHE_IMP_ENCODING_ISA_XOR

#endif
```
```c++
// CACHE ä¸­æ©ç ä½ç½®
#define CACHE_MASK_STORAGE_OUTLINED 1 // æ²¡æœ‰æ©ç 
#define CACHE_MASK_STORAGE_HIGH_16 2 // é«˜ 16 ä½
#define CACHE_MASK_STORAGE_LOW_4 3 // ä½ 4 ä½

#if defined(__arm64__) && __LP64__ // å¦‚æœæ˜¯ 64 ä½ ARM å¹³å°ï¼ˆiPhone è‡ª 5s èµ·éƒ½æ˜¯ï¼‰

// æ©ç å­˜å‚¨åœ¨é«˜ 16 ä½
#define CACHE_MASK_STORAGE CACHE_MASK_STORAGE_HIGH_16 

#elif defined(__arm64__) && !__LP64__ // ARM å¹³å° é 64 ä½ç³»ç»Ÿæ¶æ„ï¼ˆwatchOS ä¸‹ï¼‰

// æ©ç å­˜å‚¨åœ¨ä½ 4 ä½
#define CACHE_MASK_STORAGE CACHE_MASK_STORAGE_LOW_4

#else

// ä¸ä½¿ç”¨æ©ç çš„æ–¹å¼ï¼ˆ_buckets ä¸ _mask åˆ†åˆ«æ˜¯ä¸¤ä¸ªå˜é‡ï¼Œä¸Šé¢åˆ™æ˜¯æŠŠ buckets å’Œ mask åˆå¹¶ä¿å­˜åœ¨ _maskAndBuckets ä¸­ï¼‰
#define CACHE_MASK_STORAGE CACHE_MASK_STORAGE_OUTLINED

#endif
```
## bucket_t
&emsp;çœ‹åˆ° `bucket_t` ä¸€ä¸‹æƒ³èµ·äº† `RefcountMap refcnts` ä¸­ä¿å­˜å¯¹è±¡å¼•ç”¨è®¡æ•°æ—¶ä½¿ç”¨çš„æ•°æ®ç»“æ„ `typename BucketT = detail::DenseMapPair<KeyT, ValueT>>` ç”¨äºä¿å­˜å¯¹è±¡çš„åœ°å€å’Œå¯¹è±¡çš„å¼•ç”¨è®¡æ•°ã€‚`bucket_t` åŸºæœ¬ä¹Ÿæ˜¯å¤§è‡´ç›¸åŒçš„ä½œç”¨ï¼Œè¿™é‡Œæ˜¯æŠŠå‡½æ•°çš„ `SEL`  å’Œå‡½æ•°çš„å®ç°åœ°å€ `IMP` ä¿å­˜åœ¨ `bucket_t` è¿™ä¸ªç»“æ„ä½“ä¸­ã€‚è¿™é‡Œå…ˆçœ‹ä¸€ä¸‹ `bucket_t` å®šä¹‰çš„ `private` éƒ¨åˆ†:
```c++
struct bucket_t {
private:
    // ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ï¼Œé’ˆå¯¹ __arm64__ å’Œå…¶å®ƒå¹³å°è°ƒæ¢ _imp å’Œ _sel ä¸¤ä¸ªæˆå‘˜å˜é‡çš„é¡ºåº
    
    // IMP-first is better for arm64e ptrauth and no worse for arm64.
    // IMP åœ¨å‰æ—¶å¯¹ arm64e çš„æŒ‡é’ˆéªŒè¯æœºåˆ¶æ›´å¥½ï¼Œå¯¹ arm64 ä¹Ÿä¸å·®ã€‚
    
    // SEL-first is better for armv7* and i386 and x86_64.
    // SEL åœ¨å‰æ—¶å¯¹ armv7* i386 x86_64 æ›´å¥½ã€‚
    
    // typedef unsigned long uintptr_t;
    // template <typename T> struct explicit_atomic : public std::atomic<T> { ... };
    // ç¦æ­¢éšå¼è½¬æ¢ï¼ŒT æ“ä½œä¸ºåŸå­æ“ä½œï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
    
    // ç±»å‹ä¸€æ¨¡ä¸€æ ·ï¼Œè¿™é‡Œæ˜¯ä¿®æ”¹ä¸€ä¸‹ _imp å’Œ _sel çš„å‰åé¡ºåº
#if __arm64__
    explicit_atomic<uintptr_t> _imp;  
    explicit_atomic<SEL> _sel;
#else
    explicit_atomic<SEL> _sel;
    explicit_atomic<uintptr_t> _imp;
#endif
    
    // Compute the ptrauth signing modifier from &_imp, newSel, and cls.
    // ä» ï¼†_impã€newSel å’Œ cls è®¡ç®— ptrauth ç­¾åä¿®é¥°ç¬¦ã€‚
    uintptr_t modifierForSEL(SEL newSel, Class cls) const {
        // è¿ç»­å¼‚æˆ–
        return (uintptr_t)&_imp ^ (uintptr_t)newSel ^ (uintptr_t)cls;
    }

    // Sign newImp, with &_imp, newSel, and cls as modifiers.
    // ç­¾ç½² newImpï¼Œä½¿ç”¨ ï¼†_impã€newSel å’Œ cls ä½œä¿®æ”¹ã€‚
    uintptr_t encodeImp(IMP newImp, SEL newSel, Class cls) const {
        // å¦‚æœ newImp ä¸º nilï¼Œè¿”å› 0
        if (!newImp) return 0;
#if CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_PTRAUTH
    // å¦‚æœ IMP ç¼–ç ä½¿ç”¨æŒ‡é’ˆéªŒè¯æœºåˆ¶
        return (uintptr_t)
            ptrauth_auth_and_resign(newImp,
                                    ptrauth_key_function_pointer, 0,
                                    ptrauth_key_process_dependent_code,
                                    modifierForSEL(newSel, cls));
#elif CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_ISA_XOR

        // IMP ä¸ Class ä½œå¼‚æˆ–çš„å€¼
        return (uintptr_t)newImp ^ (uintptr_t)cls;
#elif CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_NONE

        // ç›´æ¥ä½¿ç”¨åŸå§‹ IMP
        return (uintptr_t)newImp;
#else

#error Unknown method cache IMP encoding. // æœªçŸ¥æ–¹å¼

#endif
    }
...
};
```
&emsp;`bucket_t` å®šä¹‰çš„ `public` éƒ¨åˆ†:
```c++
public:
    // åŸå­è¯»å– _sel
    inline SEL sel() const { return _sel.load(memory_order::memory_order_relaxed); }

    // æ ¹æ® cls ä» bucket_t å®ä¾‹ä¸­å–å¾— _imp
    inline IMP imp(Class cls) const {
        // é¦–å…ˆåŸå­è¯»å– _imp
        uintptr_t imp = _imp.load(memory_order::memory_order_relaxed);
        
        // å¦‚æœ imp ä¸å­˜åœ¨ï¼Œåˆ™è¿”å› nil
        if (!imp) return nil;
        
#if CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_PTRAUTH
        // åŸå­è¯»å– _sel
        SEL sel = _sel.load(memory_order::memory_order_relaxed);
        // è®¡ç®—è¿”å› IMP
        return (IMP)
            ptrauth_auth_and_resign((const void *)imp,
                                    ptrauth_key_process_dependent_code,
                                    modifierForSEL(sel, cls),
                                    ptrauth_key_function_pointer, 0);
#elif CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_ISA_XOR

        // imp ä¸ cls å†è¿›è¡Œä¸€æ¬¡å¼‚æˆ–ï¼Œè¿”å›åŸå€¼ï¼Œå¾—åˆ° encodeImp ä¼ å…¥çš„ newImpï¼ˆä¹‹æ‰€ä»¥æ˜¯å†ï¼Œæ˜¯å› ä¸º _imp å­˜å‚¨æ—¶å°±å·²ç»åšè¿‡ä¸€æ¬¡å¼‚æˆ–äº†ï¼‰
        return (IMP)(imp ^ (uintptr_t)cls);
        
#elif CACHE_IMP_ENCODING == CACHE_IMP_ENCODING_NONE

        // åŸå§‹ IMP
        return (IMP)imp;
#else

#error Unknown method cache IMP encoding. // æœªçŸ¥

#endif
    }

    // enum Atomicity { Atomic = true, NotAtomic = false };
    // enum IMPEncoding { Encoded = true, Raw = false };
    
    // set å‡½æ•° çš„å£°æ˜ï¼ŒæŒ‰ä½ command ç‚¹å‡» setï¼Œå¯çœ‹åˆ° set å‡½æ•°å®šä¹‰åœ¨ objc-cache.mm ä¸­
    template <Atomicity, IMPEncoding>
    void set(SEL newSel, IMP newImp, Class cls);
};
```
### set
&emsp;`set` å‡½æ•°å®Œæˆçš„åŠŸèƒ½æ˜¯ä»¥åŸå­æ–¹å¼å®Œæˆ `bucket_t` å®ä¾‹ `_imp` å’Œ `_sel` æˆå‘˜å˜é‡çš„è®¾ç½®ã€‚ 

`memory_order` çš„å€¼å¯å‚è€ƒ: [ã€Šå¦‚ä½•ç†è§£ C++11 çš„å…­ç§ memory orderï¼Ÿã€‹](https://www.zhihu.com/question/24301047)
```c++
// é __arm64__ å¹³å°ä¸‹(x86_64 ä¸‹):
template<Atomicity atomicity, IMPEncoding impEncoding>
void bucket_t::set(SEL newSel, IMP newImp, Class cls)
{
    // å½“å‰ bucket_t å®ä¾‹çš„ _sel ä¸º 0 æˆ–è€…ä¸ä¼ å…¥çš„ newSel ç›¸åŒ
    // DEBUG ä¸‹ bucket_t çš„ _sel å’Œ newSel ä¸åŒå°±ä¼šæ‰§è¡Œæ–­è¨€ ï¼Ÿ
    
    ASSERT(_sel.load(memory_order::memory_order_relaxed) == 0 ||
           _sel.load(memory_order::memory_order_relaxed) == newSel);

    // objc_msgSend uses sel and imp with no locks.
    // objc_msgSend ä½¿ç”¨ sel å’Œ imp ä¸ä¼šåŠ é”ã€‚
    
    // It is safe for objc_msgSend to see new imp but NULL sel 
    // objc_msgssend å¯ä»¥å®‰å…¨åœ°çœ‹åˆ°æ–°çš„ imp å’Œ NULL çš„ selã€‚
    
    // (It will get a cache miss but not dispatch to the wrong place.
    //  å®ƒå°†å¯¼è‡´ç¼“å­˜æœªå‘½ä¸­ï¼Œä½†ä¸ä¼šåˆ†æ´¾åˆ°é”™è¯¯çš„ä½ç½®ã€‚)
    
    // It is unsafe for objc_msgSend to see old imp and new sel.
    // objc_msgSend æŸ¥çœ‹æ—§çš„ imp å’Œæ–°çš„ sel æ˜¯ä¸å®‰å…¨çš„ã€‚
    
    // Therefore we write new imp, wait a lot, then write new sel.
    // å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå†™å…¥æ–°çš„ impï¼Œç­‰ä¸€ä¸‹ï¼Œç„¶åå†å†™å…¥æ–°çš„ selã€‚
    
    // æ ¹æ® impEncoding åˆ¤æ–­ æ–° IMP æ˜¯éœ€è¦åšå¼‚æˆ–æ±‚å€¼è¿˜æ˜¯ç›´æ¥ä½¿ç”¨
    uintptr_t newIMP = (impEncoding == Encoded
                        ? encodeImp(newImp, newSel, cls)
                        : (uintptr_t)newImp);

    if (atomicity == Atomic) {
        // å¦‚æœæ˜¯ Atomic
        // é¦–å…ˆæŠŠ newIMP å­˜å‚¨åˆ° _imp
        _imp.store(newIMP, memory_order::memory_order_relaxed);
        
        // _sel æ˜¯ 0 æ—¶ï¼š
        if (_sel.load(memory_order::memory_order_relaxed) != newSel) {
        // å¦‚æœå½“å‰ _sel ä¸ newSel ä¸åŒï¼Œåˆ™æ ¹æ®ä¸åŒçš„å¹³å°æ¥è®¾ç½® _sel
        
#ifdef __arm__
            // barrier
            mega_barrier();
            _sel.store(newSel, memory_order::memory_order_relaxed);
#elif __x86_64__ || __i386__
            _sel.store(newSel, memory_order::memory_order_release);
#else

// ä¸çŸ¥é“å¦‚ä½•åœ¨æ­¤æ¶æ„ä¸Šæ‰§è¡Œ bucket_t::setã€‚
#error Don't know how to do bucket_t::set on this architecture.

#endif
        }
    } else {
        // åŸå­ä¿å­˜ _imp
        _imp.store(newIMP, memory_order::memory_order_relaxed);
        // åŸå­ä¿å­˜ _sel
        _sel.store(newSel, memory_order::memory_order_relaxed);
    }
}
```
&emsp;é¦–å…ˆè¦æŠŠ `newImp` å†™å…¥ï¼Œ`__arm64__` ä¸‹ `set` å‡½æ•°çš„å®ç°æ¶‰åŠä¸€ä¸ª `__asm__` å¥½åƒæ¶‰åŠåˆ° `ARM` çš„å†…å­˜æ’åºå†…å­˜å±éšœå•¥çš„çœ‹ä¸æ‡‚ã€‚
`struct bucket_t` åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œä¸»è¦ç”¨æ¥ä¿å­˜å‡½æ•°çš„ `SEL` å’Œ `IMP`ï¼ˆ`IMP` æ ¹æ®ä¸åŒçš„ç¼–ç æ–¹å¼æ¥ä¿å­˜ï¼‰ã€‚ 

## cache_t
&emsp;`cache_t` æ˜¯ä½œä¸ºä¸€ä¸ªæ•£åˆ—æ•°ç»„æ¥ç¼“å­˜æ–¹æ³•çš„ã€‚å…ˆçœ‹ä¸‹ `cache_t` å®šä¹‰çš„ `private` éƒ¨åˆ†:

### mask_t:
```c++
#if __LP64__

// x86_64 & arm64 asm are less efficient with 16-bits
// x86_64 å’Œ arm64 asm çš„ 16 ä½æ•ˆç‡è¾ƒä½

typedef uint32_t mask_t; // 32 ä½ 4 å­—èŠ‚ int
#else

typedef uint16_t mask_t; // 16 ä½ 2 å­—èŠ‚ int
#endif
```
### struct cache_t private
```c++
struct cache_t {
#if CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_OUTLINED
    // å¦‚æœæ²¡æœ‰æ©ç çš„è¯
    
    // _buckets æ˜¯ struct bucket_t ç±»å‹çš„æ•°ç»„
    // æ–¹æ³•çš„ç¼“å­˜æ•°ç»„ï¼ˆä»¥æ•£åˆ—è¡¨çš„å½¢å¼å­˜å‚¨ bucket_tï¼‰
    explicit_atomic<struct bucket_t *> _buckets;
    
    // _buckets çš„æ•°ç»„é•¿åº¦ -1ï¼ˆå®¹é‡çš„ä¸´ç•Œå€¼ï¼‰
    explicit_atomic<mask_t> _mask;
    
#elif CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_HIGH_16
    // é«˜ 16 ä½æ˜¯æ©ç çš„å¹³å°ï¼ˆiPhone 5s ä»¥åçš„çœŸæœºï¼‰
    
    // æ©ç å’Œ Buckets æŒ‡é’ˆå…±åŒä¿å­˜åœ¨ uintptr_t ç±»å‹çš„ _maskAndBuckets ä¸­
    explicit_atomic<uintptr_t> _maskAndBuckets;
    
    mask_t _mask_unused; // æœªä½¿ç”¨çš„å®¹é‡
    
    // How much the mask is shifted by.
    // é«˜ 16 ä½æ˜¯ maskï¼Œå³ _maskAndBuckets å³ç§» 48 ä½å¾—åˆ° mask
    static constexpr uintptr_t maskShift = 48;
    
    // Additional bits after the mask which must be zero. 
    // msgSend takes advantage of these additional bits to construct the value `mask << 4` from `_maskAndBuckets` in a single instruction.
    // æ©ç åçš„å…¶ä»–ä½å¿…é¡»ä¸ºé›¶ã€‚
    // msgSend åˆ©ç”¨è¿™äº›é¢å¤–çš„ä½åœ¨å•ä¸ªæŒ‡ä»¤ä¸­ä» _maskAndBuckets æ„é€ äº†å€¼ mask<< 4
    static constexpr uintptr_t maskZeroBits = 4;
    
    // The largest mask value we can store.
    // æˆ‘ä»¬å¯ä»¥ä¿å­˜çš„æœ€å¤§çš„ mask å€¼ã€‚
    // (64 - maskShiift) å³æ©ç ä½æ•°ï¼Œç„¶å 1 å·¦ç§»æ©ç ä½æ•°åå† å‡ 1 å³ 16 ä½èƒ½ä¿å­˜çš„æœ€å¤§äºŒè¿›åˆ¶å€¼
    //ï¼ˆ16 ä½ 1ï¼Œå…¶ä½™ä½éƒ½æ˜¯ 0 çš„æ•°å€¼ï¼‰
    static constexpr uintptr_t maxMask = ((uintptr_t)1 << (64 - maskShift)) - 1;
    
    // The mask applied to `_maskAndBuckets` to retrieve the buckets pointer.
    // åº”ç”¨äº _maskAndBuckets çš„æ©ç ï¼Œä»¥è·å– buckets æŒ‡é’ˆã€‚
    // 1 å·¦ç§» 44(48 - 4) ä½åå† å‡ 1ï¼ˆ44 ä½ 1ï¼Œå…¶ä½™éƒ½æ˜¯ 0 çš„æ•°å€¼ï¼‰
    static constexpr uintptr_t bucketsMask = ((uintptr_t)1 << (maskShift - maskZeroBits)) - 1;
    
    // Ensure we have enough bits for the buckets pointer.
    // ç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„ä½ç”¨äºå­˜å‚¨ buckets æŒ‡é’ˆã€‚
    static_assert(bucketsMask >= MACH_VM_MAX_ADDRESS, "Bucket field doesn't have enough bits for arbitrary pointers.");
    
#elif CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_LOW_4
    // _maskAndBuckets stores the mask shift in the low 4 bits, and the buckets pointer in the remainder of the value. 
    // The mask shift is the value where (0xffff >> shift) produces the correct mask.
    // This is equal to 16 - log2(cache_size).
    // _maskAndBuckets å°†æ©ç ç§»ä½å­˜å‚¨åœ¨ä½ 4 ä½ä¸­ï¼Œå¹¶å°† buckets æŒ‡é’ˆå­˜å‚¨åœ¨è¯¥å€¼çš„å…¶ä½™éƒ¨åˆ†ä¸­ã€‚
    // æ©ç  shift æ˜¯ï¼ˆ0xffff >> shiftï¼‰äº§ç”Ÿæ­£ç¡®æ©ç çš„å€¼ã€‚
    // ç­‰äº 16 - log2(cache_size)
    
    // å‡ ä¹åŒä¸Š
    explicit_atomic<uintptr_t> _maskAndBuckets;
    mask_t _mask_unused;

    static constexpr uintptr_t maskBits = 4;
    static constexpr uintptr_t maskMask = (1 << maskBits) - 1;
    static constexpr uintptr_t bucketsMask = ~maskMask;
#else

#error Unknown cache mask storage type. // æœªçŸ¥æ©ç å­˜å‚¨ç±»å‹

#endif

#if __LP64__
    // å¦‚æœæ˜¯ 64 ä½ç¯å¢ƒçš„è¯ä¼šå¤šä¸€ä¸ª _flags æ ‡å¿—ä½
    uint16_t _flags;
#endif

    uint16_t _occupied; // ç¼“å­˜æ•°ç»„çš„å·²å ç”¨é‡
...
};
```
### struct cache_t public
&emsp;`cache_t` å®šä¹‰çš„ `public` éƒ¨åˆ†: `cache_t` çš„å®ç°éƒ¨åˆ†ä¹Ÿæ˜¯æ¶‰åŠåˆ°ä¸åŒçš„å¹³å°ä¸‹ä¸åŒçš„å®ç°ï¼Œè¿™é‡Œåªåˆ†æ `CACHE_MASK_STORAGE_OUTLINED(x86_64)` å’Œ `CACHE_MASK_STORAGE_HIGH_16 (__arm64__ && __LP64__)` ä¸¤ä¸ªå¹³å°çš„å®ç°ã€‚

#### emptyBuckets
&emsp;ä¸€ä¸ªæŒ‡å‘ `_objc_empty_cache` çš„ `bucket_t` æŒ‡é’ˆï¼Œç”¨æ¥æŒ‡ç¤ºå½“å‰ç±»çš„ç¼“å­˜æŒ‡å‘ç©ºç¼“å­˜ã€‚ï¼ˆ`_objc_empty_cache` æ˜¯ä¸€ä¸ªå¤–è”å˜é‡ï¼‰
```c++
// OBJC2 ä¸å¯è§
struct objc_cache {
    unsigned int mask /* total = mask + 1 */                 OBJC2_UNAVAILABLE;
    unsigned int occupied                                    OBJC2_UNAVAILABLE;
    Method _Nullable buckets[1]                              OBJC2_UNAVAILABLE;
};

OBJC_EXPORT struct objc_cache _objc_empty_cache OBJC_AVAILABLE(10.0, 2.0, 9.0, 1.0, 2.0);

// ä½äº objc-cache-old.mm ä¸­
// é™æ€çš„ç©ºç¼“å­˜ï¼Œæ‰€æœ‰ç±»æœ€åˆéƒ½æŒ‡å‘æ­¤ç¼“å­˜ã€‚
// å‘é€ç¬¬ä¸€æ¡æ¶ˆæ¯æ—¶ï¼Œå®ƒåœ¨ç¼“å­˜ä¸­æœªå‘½ä¸­ï¼Œå½“ç¼“å­˜æ–°å¢æ—¶ï¼Œå®ƒä¼šæ£€æŸ¥è¿™ç§æƒ…å†µï¼Œå¹¶ä½¿ç”¨ malloc è€Œä¸æ˜¯ reallocã€‚
// è¿™é¿å…äº†åœ¨ Messenger ä¸­æ£€æŸ¥ NULL ç¼“å­˜çš„éœ€è¦ã€‚
struct objc_cache _objc_empty_cache =
{
    0,        // mask
    0,        // occupied
    { NULL }  // buckets
};

// CACHE_MASK_STORAGE_OUTLINED ä¸‹
struct bucket_t *cache_t::emptyBuckets()
{
    // ç›´æ¥ä½¿ç”¨ & å– _objc_empty_cache çš„åœ°å€å¹¶è¿”å›ï¼Œ
    // _objc_empty_cache æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œç”¨æ¥æ ‡è®°å½“å‰ç±»çš„ç¼“å­˜æ˜¯ä¸€ä¸ªç©ºç¼“å­˜ã€‚
    return (bucket_t *)&_objc_empty_cache;
}

// CACHE_MASK_STORAGE_HIGH_16 ä¸‹ï¼ˆçœ‹åˆ°å’Œ OUTLINED å®Œå…¨ä¸€æ ·ï¼‰
struct bucket_t *cache_t::emptyBuckets()
{
    return (bucket_t *)&_objc_empty_cache;
}
```
#### buckets
&emsp;æ•£åˆ—æ•°ç»„çš„èµ·å§‹åœ°å€ã€‚
```c++
// CACHE_MASK_STORAGE_OUTLINED ä¸‹
// æ²¡æœ‰ä»»ä½•å—¦å¤´ï¼ŒåŸå­åŠ è½½ _buckets å¹¶è¿”å›
struct bucket_t *cache_t::buckets() 
{
    // åŸå­åŠ è½½ _buckets å¹¶è¿”å›
    return _buckets.load(memory_order::memory_order_relaxed);
}

// CACHE_MASK_STORAGE_HIGH_16 ä¸‹
// ä¹Ÿæ˜¯æ²¡æœ‰ä»»ä½•å—¦å¤´ï¼ŒåŸå­åŠ è½½ _maskAndBucketsï¼Œç„¶åä¸ bucketsMask æ©ç ä¸æ“ä½œå¹¶æŠŠç»“æœè¿”å›
struct bucket_t *cache_t::buckets()
{
    // åŸå­åŠ è½½ _maskAndBuckets
    uintptr_t maskAndBuckets = _maskAndBuckets.load(memory_order::memory_order_relaxed);
    // ç„¶åä¸ bucketsMask åšä¸æ“ä½œå¹¶è¿”å›ç»“æœã€‚
    //ï¼ˆbucketsMask çš„å€¼æ˜¯ä½ 44 ä½æ˜¯ 1ï¼Œå…¶å®ƒä½å…¨éƒ¨æ˜¯ 0ï¼Œä¸æ“ä½œå–å‡º maskAndBuckets ä½ 44 ä½çš„å€¼ï¼‰
    return (bucket_t *)(maskAndBuckets & bucketsMask);
}
```

#### mask
&emsp;`_buckets` çš„æ•°ç»„é•¿åº¦ -1ï¼ˆå®¹é‡çš„ä¸´ç•Œå€¼ï¼‰ã€‚
```c++
// CACHE_MASK_STORAGE_OUTLINED
mask_t cache_t::mask() 
{
    // æ²¡æœ‰ä»»ä½•å—¦å¤´ï¼ŒåŸå­åŠ è½½ _mask å¹¶è¿”å›
    return _mask.load(memory_order::memory_order_relaxed);
}

// CACHE_MASK_STORAGE_HIGH_16
mask_t cache_t::mask()
{
    // åŸå­åŠ è½½ _maskAndBuckets
    uintptr_t maskAndBuckets = _maskAndBuckets.load(memory_order::memory_order_relaxed);
    // maskAndBuckets å·¦ç§» 48 ä½å³å¾—åˆ°äº† maskï¼Œå¹¶è¿”å›æ­¤å€¼ã€‚ï¼ˆé«˜ 16 ä½ä¿å­˜ maskï¼‰
    return maskAndBuckets >> maskShift;
}

#if __LP64__
typedef uint32_t mask_t;  // x86_64 & arm64 asm are less efficient with 16-bits
#else
typedef uint16_t mask_t;
#endif

typedef uintptr_t SEL;
```
&emsp;è¿™é‡Œæœ‰ä¸€ä¸ªç‚¹ï¼Œåœ¨ `CACHE_MASK_STORAGE_HIGH_16` æ—¶æ˜¯ `__LP64__` å¹³å°ï¼Œ`mask_t` åœ¨ `__LP64__` ä¸‹æ˜¯ `uint32_t`ï¼Œå¤šå‡ºäº† `16` ä½ç©ºé—´ï¼Œ`mask` åªéœ€è¦ `16` ä½å°±è¶³å¤Ÿä¿å­˜ã€‚æ³¨é‡Šç»™å‡ºçš„è§£é‡Šæ˜¯: " `x86_64` å’Œ `arm64` `asm` çš„ `16` ä½æ•ˆç‡è¾ƒä½ã€‚"

#### occupied/incrementOccupied
```c++
mask_t cache_t::occupied() 
{
    return _occupied; // è¿”å› _occupied
}

void cache_t::incrementOccupied() 
{
    _occupied++; // _occupied è‡ªå¢
}
```
#### setBucketsAndMask
&emsp;è®¾ç½® `_buckets` ä¸ `_mask` çš„å€¼ï¼Œ`CACHE_MASK_STORAGE_OUTLINED` æ¨¡å¼åªéœ€è¦åˆ†åˆ«ä»¥åŸå­æ–¹å¼è®¾ç½®ä¸¤ä¸ªæˆå‘˜å˜é‡çš„å€¼å³å¯ï¼Œ`CACHE_MASK_STORAGE_HIGH_16` æ¨¡å¼éœ€è¦æŠŠä¸¤ä¸ªå€¼åšä½æ“ä½œåˆå¹¶åœ¨ä¸€èµ·ç„¶åä»¥åŸå­æ–¹å¼ä¿å­˜åœ¨ `_maskAndBuckets` ä¸­ã€‚åŒæ—¶ä»¥ä¸Šä¸¤ç§æƒ…å†µéƒ½ä¼šé¡ºä¾¿æŠŠ `_occupied` è®¾ç½®ä¸º `0`ã€‚
```c++
// CACHE_MASK_STORAGE_OUTLINED
void cache_t::setBucketsAndMask(struct bucket_t *newBuckets, mask_t newMask)
{
    // objc_msgSend uses mask and buckets with no locks.
    // objc_msgSend ä½¿ç”¨ mask å’Œ buckets ä¸ä¼šè¿›è¡ŒåŠ é”ã€‚
    
    // It is safe for objc_msgSend to see new buckets but old mask.
    // å¯¹äº objc_msgSend æ¥è¯´ï¼Œçœ‹åˆ°æ–°çš„ buckets å’Œæ—§çš„ mask æ˜¯å®‰å…¨çš„ã€‚
    
    // (It will get a cache miss but not overrun the buckets' bounds).
    // (å®ƒå°†è·å¾—ç¼“å­˜æœªå‘½ä¸­ï¼Œä½†ä¸ä¼šè¶…å‡ºå­˜å‚¨æ¡¶çš„ç•Œé™ã€‚)
    
    // It is unsafe for objc_msgSend to see old buckets and new mask.
    // objc_msgSend æŸ¥çœ‹æ—§ buckets å’Œæ–° mask æ˜¯ä¸å®‰å…¨çš„ã€‚
    
    // Therefore we write new buckets, wait a lot, then write new mask.
    // æ‰€ä»¥æˆ‘ä»¬å…ˆå†™å…¥æ–°çš„ bucketsï¼Œå†™å…¥å®Œæˆåï¼Œå†å†™å…¥æ–°çš„ maskã€‚
    
    // objc_msgSend reads mask first, then buckets.
    // objc_msgSend é¦–å…ˆè¯»å– maskï¼Œç„¶åè¯»å– bucketsã€‚

#ifdef __arm__
    // ensure other threads see buckets contents before buckets pointer
    // ç¡®ä¿å…¶ä»–çº¿ç¨‹åœ¨ buckets æŒ‡é’ˆä¹‹å‰çœ‹åˆ° buckets å†…å®¹
    mega_barrier();

    _buckets.store(newBuckets, memory_order::memory_order_relaxed);
    
    // ensure other threads see new buckets before new mask
    // ç¡®ä¿å…¶ä»–çº¿ç¨‹åœ¨æ–° mask ä¹‹å‰çœ‹åˆ°æ–° buckets
    mega_barrier();
    
    _mask.store(newMask, memory_order::memory_order_relaxed);
    _occupied = 0; // _occupied ç½®ä¸º 0
#elif __x86_64__ || i386
    // ensure other threads see buckets contents before buckets pointer
    // ç¡®ä¿å…¶ä»–çº¿ç¨‹åœ¨ buckets æŒ‡é’ˆä¹‹å‰çœ‹åˆ° buckets å†…å®¹
    // ä»¥åŸå­æ–¹å¼ä¿å­˜ _buckets
    _buckets.store(newBuckets, memory_order::memory_order_release);
    
    // ensure other threads see new buckets before new mask
    // ç¡®ä¿å…¶ä»–çº¿ç¨‹åœ¨æ–° mask ä¹‹å‰çœ‹åˆ°æ–° buckets
    // ä»¥åŸå­æ–¹å¼ä¿å­˜ _mask
    _mask.store(newMask, memory_order::memory_order_release);
    _occupied = 0; // _occupied ç½®ä¸º 0
#else

// ä¸çŸ¥é“å¦‚ä½•åœ¨æ­¤æ¶æ„ä¸Šæ‰§è¡Œ setBucketsAndMaskã€‚
#error Don't know how to do setBucketsAndMask on this architecture.

#endif
}

// CACHE_MASK_STORAGE_HIGH_16
void cache_t::setBucketsAndMask(struct bucket_t *newBuckets, mask_t newMask)
{
    // è½¬ä¸º unsigned long
    uintptr_t buckets = (uintptr_t)newBuckets;
    uintptr_t mask = (uintptr_t)newMask;
    
    // æ–­è¨€: buckets å°äºç­‰äº buckets çš„æ©ç ï¼ˆbucketsMask çš„å€¼ä½ 44 ä½å…¨ä¸º 1ï¼Œå…¶å®ƒä½æ˜¯ 0ï¼‰
    ASSERT(buckets <= bucketsMask);
    // æ–­è¨€: mask å°äºç­‰äº mask çš„æœ€å¤§å€¼ï¼ˆmaxMask çš„å€¼ä½ 16 ä½å…¨ä¸º 1ï¼Œå…¶å®ƒä½æ˜¯ 0ï¼‰
    ASSERT(mask <= maxMask);
    
    // newMask å·¦ç§» 48 ä½ç„¶åä¸ newBuckets åšæˆ–æ“ä½œï¼Œ
    // å› ä¸º newBuckets é«˜ 16 ä½å…¨éƒ¨æ˜¯ 0ï¼Œæ‰€ä»¥ newMask å·¦ç§» 16 çš„å€¼ä¸ newBuckets åšæˆ–æ“ä½œæ—¶ä¾ç„¶ä¿æŒä¸å˜
    // æŠŠç»“æœä»¥åŸå­æ–¹å¼ä¿å­˜åœ¨ _maskAndBuckets ä¸­  
    _maskAndBuckets.store(((uintptr_t)newMask << maskShift) | (uintptr_t)newBuckets, std::memory_order_relaxed);
    // æŠŠ _occupied ç½®ä¸º 0
    _occupied = 0;
}
```
#### initializeToEmpty
```c++
// bzero
// å¤´æ–‡ä»¶ï¼š#include <string.h>
// å‡½æ•°åŸå‹ï¼švoid bzero (void *s, int n);
// åŠŸèƒ½ï¼šå°†å­—ç¬¦ä¸² s çš„å‰ n ä¸ªå­—èŠ‚ç½®ä¸º 0ï¼Œä¸€èˆ¬æ¥è¯´ n é€šå¸¸å– sizeof(s)ï¼Œå°†æ•´å—ç©ºé—´æ¸…é›¶

// CACHE_MASK_STORAGE_OUTLINED
void cache_t::initializeToEmpty()
{
    // æŠŠ this çš„å†…å­˜æ¸…é›¶
    bzero(this, sizeof(*this));
    // ä»¥åŸå­æ–¹å¼æŠŠ _objc_empty_cache çš„åœ°å€å­˜å‚¨åœ¨ _buckets ä¸­
    _buckets.store((bucket_t *)&_objc_empty_cache, memory_order::memory_order_relaxed);
}

// CACHE_MASK_STORAGE_HIGH_16
void cache_t::initializeToEmpty()
{
    // æŠŠ this çš„å†…å­˜æ¸…é›¶
    bzero(this, sizeof(*this));
    // æŠŠ _objc_empty_cache çš„åœ°å€è½¬æ¢ä¸º uintptr_tç„¶åä»¥åŸå­æ–¹å¼æŠŠå…¶å­˜å‚¨åœ¨ _maskAndBuckets ä¸­ 
    _maskAndBuckets.store((uintptr_t)&_objc_empty_cache, std::memory_order_relaxed);
}
```
&emsp;ä¸¤ç§æ¨¡å¼ä¸‹éƒ½æ˜¯æŠŠ `_objc_empty_cache` çš„åœ°å€å–å‡ºç”¨äºè®¾ç½® `_buckets/_maskAndBuckets`ï¼Œä¸¤ç§æ¨¡å¼ä¸‹ä¹Ÿéƒ½å¯¹åº”ä¸Šé¢çš„ `emptyBuckets` å‡½æ•°ï¼Œå–å‡º `(bucket_t *)&_objc_empty_cache` è¿”å›ã€‚  
#### canBeFreed
&emsp;`canBeFreed` å‡½æ•°åªæœ‰ä¸‹é¢ä¸€ç§å®ç°ï¼Œçœ‹åå­—æˆ‘ä»¬å¤§æ¦‚ä¹Ÿèƒ½çŒœå‡ºæ­¤å‡½æ•°çš„ä½œç”¨ï¼Œæ­£å¼åˆ¤æ–­èƒ½ä¸èƒ½é‡Šæ”¾ `cache_t`ã€‚
```c++
bool cache_t::canBeFreed()
{
    // è°ƒç”¨ isConstantEmptyCache å‡½æ•°ï¼Œå¦‚æœå®ƒè¿”å› trueï¼Œ
    // åˆ™è¡¨æ˜ cache_t çš„ buckets å½“å‰æ­£æ˜¯é‚£äº›å‡†å¤‡çš„æ ‡è®° emptyBuckets çš„é™æ€å€¼ã€‚
    
    //ï¼ˆå½“ capacity å°äº capacity æ—¶ï¼Œ
    // isConstantEmptyCache å‡½æ•°å†…éƒ¨çš„ emptyBucketsForCapacity å‡½æ•°è¿”å›çš„éƒ½æ˜¯:
    // cache_t::emptyBuckets() å…¨å±€çš„ (bucket_t *)&_objc_empty_cache å€¼ï¼‰ï¼Œåˆ™ä¸èƒ½è¿›è¡Œé‡Šæ”¾ï¼Œ
    
    // æˆ‘ä»¬è‡ªå·±ç”³è¯·çš„æœ‰æ•ˆçš„æ–¹æ³•ç¼“å­˜å†…å®¹ï¼Œæ‰å¯æ ¹æ®æƒ…å†µè¿›è¡Œé‡Šæ”¾ã€‚
    
    return !isConstantEmptyCache();
}
```
#### isConstantEmptyCache
&emsp;çœ‹å®Œä¸‹é¢çš„ `emptyBucketsForCapacity` å®ç°æ‰çŸ¥é“ `isConstantEmptyCache` ä¸­ `Constant` çš„å«ä¹‰ã€‚
```c++
bool cache_t::isConstantEmptyCache()
{
    // occupied() å‡½æ•°å¾ˆç®€å•å°±æ˜¯è·å– _occupied æˆå‘˜å˜é‡çš„å€¼ç„¶åç›´æ¥è¿”å›ï¼Œ
    // _occupied è¡¨ç¤ºæ•£åˆ—è¡¨ä¸­å·²å ç”¨çš„å®¹é‡
    // æ­¤å¤„è¦æ±‚ occupied() ä¸º 0 å¹¶ä¸” buckets() ç­‰äº emptyBucketsForCapacity(capacity(), false)
    // emptyBucketsForCapacity å‡½æ•°åˆ™æ˜¯æ ¹æ® capacity() å»æ‰¾å…¶å¯¹åº”çš„ emptyBucketsï¼Œ
    // ä¸”è¿™äº› emptyBuckets åœ°å€éƒ½æ˜¯å›ºå®šçš„ï¼Œ
    // å®ƒä»¬æ˜¯ä½œæ ‡è®°ç”¨çš„é™æ€å€¼ï¼Œå¦‚æœæ­¤æ—¶ buckets æ­£æ˜¯è¿™äº›ä¸ªé™æ€å€¼ï¼Œè¯´æ˜æ­¤æ—¶ cache_t æ˜¯ä¸€ä¸ªç©ºç¼“å­˜ã€‚
    return 
        occupied() == 0  &&  
        buckets() == emptyBucketsForCapacity(capacity(), false); 
        // ä¸”è¿™é‡Œç”¨äº† false åˆ™ä¸‹é¢ä¸æ‰§è¡Œ emptyBucketsList ç›¸å…³çš„ç”³è¯·ç©ºé—´çš„é€»è¾‘ï¼Œ
        // ä¼šç›´æ¥ if (!allocate) return nil;
}
```
#### capacity
```c++
// mask æ˜¯ä¸´ç•Œå€¼ï¼ŒåŠ  1 åå°±æ˜¯æ•£åˆ—è¡¨çš„å®¹é‡
unsigned cache_t::capacity()
{
    return mask() ? mask()+1 : 0; 
}
```
### emptyBucketsForCapacity
&emsp;æ ¹æ®å…¥å‚ `capacity`ï¼Œè¿”å›ä¸€ä¸ªæŒ‡å®š `capacity` å®¹é‡çš„ç©ºçš„æ•£åˆ—è¡¨ï¼Œè¿”å›çš„è¿™ä¸ª `bucket_t *` æ˜¯  `static bucket_t **emptyBucketsList` è¿™ä¸ªé™æ€å˜é‡æŒ‡å®šä¸‹æ ‡çš„å€¼ï¼Œå½“ `capacity` ä½äºæŒ‡å®šçš„åŒºé—´æ—¶ï¼Œè¿”å›çš„ `bucket_t *` éƒ½æ˜¯ç›¸åŒçš„ã€‚å¦‚æœ `capacity` è¶…å‡ºäº†ç°æœ‰çš„å®¹é‡ç•Œé™ï¼Œåˆ™ä¼šå¯¹ `emptyBucketsList` è¿›è¡Œæ‰©å®¹ã€‚
ä¾‹å¦‚ï¼š`capacity` å€¼åœ¨ `[8ï¼Œ15]` ä¹‹å†…æ—¶ï¼Œé€šè¿‡ `index = log2u(capacity)` è®¡ç®—çš„ `index` å€¼éƒ½æ˜¯ç›¸åŒçš„ï¼Œé‚£ä¹ˆè°ƒç”¨ `emptyBucketsForCapacity` å‡½æ•°è¿”å›çš„éƒ½æ˜¯ç›¸åŒçš„ `emptyBucketsList[index]`ã€‚ç”±äºè¿™é‡Œæœ‰ `EMPTY_BYTES` é™åˆ¶ï¼Œæ‰€ä»¥è‡³å°‘ `capacity`  å¤§äº `9/1025` æ‰ä¼šä½¿ç”¨åˆ° `emptyBucketsList` ç›¸å…³çš„é€»è¾‘ï¼Œå†…éƒ¨ `index` æ˜¯ä» `3/10`ï¼ˆ2 çš„ 3 æ¬¡æ–¹æ˜¯ 8ï¼Œ2 çš„ 10 æ¬¡æ–¹æ˜¯ 1024ï¼‰ å¼€å§‹çš„ã€‚å…¶å®ƒçš„æƒ…å†µåˆ™ä¸€å¾‹è¿”å› `cache_t::emptyBuckets()`ã€‚
```c++
bucket_t *emptyBucketsForCapacity(mask_t capacity, bool allocate = true)
{
#if CONFIG_USE_CACHE_LOCK
    cacheUpdateLock.assertLocked();
#else
    runtimeLock.assertLocked(); // èµ°æ­¤åˆ†æ”¯ï¼ŒåŠ é”ï¼ˆåŠ é”å¤±è´¥ä¼šæ‰§è¡Œæ–­è¨€ï¼‰
#endif

    // buckets æ€»å­—èŠ‚å ç”¨ï¼ˆsizeof(bucket_t) * capacityï¼‰
    size_t bytes = cache_t::bytesForCapacity(capacity);
    
    // Use _objc_empty_cache if the buckets is small enough.
    // å¦‚æœ buckets è¶³å¤Ÿå°çš„è¯ä½¿ç”¨ _objc_empty_cacheã€‚
    if (bytes <= EMPTY_BYTES) {
        // å°äº ((8+1)*16) ä¸»è¦é’ˆå¯¹çš„æ˜¯ DEBUG æ¨¡å¼ä¸‹ã€‚
        // å°äº ((1024+1)*16) é DEBUG æ¨¡å¼ï¼ˆåé¢çš„ä¹˜ä»¥ 16 æ˜¯å› ä¸º sizeof(bucket_t) == 16ï¼‰
        //ï¼ˆè§‰å¾—è¿™ä¸ª 1025 çš„å®¹é‡å°±å·²ç»å¾ˆå¤§å¤§äº†ï¼Œå¯èƒ½å¾ˆéš¾è¶…è¿‡ï¼Œå¤§æ¦‚ç‡è¿™é‡Œå°±ç›´æ¥è¿”å› cache_t::emptyBuckets() äº†ï¼‰
        return cache_t::emptyBuckets();
    }

    // Use shared empty buckets allocated on the heap.
    // ä½¿ç”¨åœ¨å †ä¸Šåˆ†é…çš„ shared empty bucketsã€‚
    
    // é™æ€çš„ bucket_t **ï¼Œä¸‹æ¬¡å†è¿›å…¥ emptyBucketsForCapacity å‡½æ•°çš„è¯ä¾ç„¶æ˜¯ä¿æŒä¸Šæ¬¡çš„å€¼
    // ä¸”è¿”å›å€¼æ­£æ˜¯ emptyBucketsList[index]ï¼Œå°±æ˜¯è¯´è°ƒç”¨ emptyBucketsForCapacity è·å–å°±æ˜¯ä¸€ä¸ªé™æ€çš„å®šå€¼
    static bucket_t **emptyBucketsList = nil;
    
    // é™æ€çš„ mask_t (uint32_t)ï¼Œä¸‹æ¬¡å†è¿›å…¥ emptyBucketsForCapacity å‡½æ•°çš„è¯ä¾ç„¶æ˜¯ä¿æŒä¸Šæ¬¡çš„å€¼
    static mask_t emptyBucketsListCount = 0;
    
    // âš ï¸
    // template <typename T>
    // static inline T log2u(T x) {
    //     return (x<2) ? 0 : log2u(x>>1)+1;
    // }
    
    // log2u è®¡ç®—çš„æ˜¯å°äºç­‰äº x çš„æœ€å¤§çš„ 2 å¹‚çš„æŒ‡æ•°
    // x åœ¨ [8ï¼Œ15] åŒºé—´å†…ï¼Œå¤§äºç­‰äº 2^3ï¼Œæ‰€ä»¥è¿”å›å€¼ä¸º 3
    // x åœ¨ [16, 31] åŒºé—´å†…ï¼Œå¤§äºç­‰äº 2^4, æ‰€ä»¥è¿”å›å€¼ä¸º 4
    
    mask_t index = log2u(capacity);

    if (index >= emptyBucketsListCount) {
        if (!allocate) return nil;

        // index + 1 æ­¤å€¼è¿˜æ²¡æœ‰çœ‹å‡ºæ¥æ˜¯ä»€ä¹ˆæ„æ€
        mask_t newListCount = index + 1;
        
        // capacity å¤§äº 9/1026 é‚£ä¹ˆ bytes å¤§äº 9 * 16/1026 * 16ï¼Œ16 Kb ä¹Ÿå¯å¤ªå¤§äº†
        // åˆ†é… bytes ä¸ªé•¿åº¦ä¸º 1 çš„è¿ç»­å†…å­˜ç©ºé—´ï¼Œä¸”å†…å­˜åˆå§‹åŒ–ä¸º 0
        bucket_t *newBuckets = (bucket_t *)calloc(bytes, 1);
        
        // âš ï¸
        // extern void *realloc(void *mem_address, unsigned int newsize);
        //ï¼ˆæ•°æ®ç±»å‹*ï¼‰reallocï¼ˆè¦æ”¹å˜å†…å­˜å¤§å°çš„æŒ‡é’ˆåï¼Œæ–°çš„å¤§å°ï¼‰
        // æ–°çš„å¤§å°å¯å¤§å¯å°ï¼Œå¦‚æœæ–°çš„å¤§å°å¤§äºåŸå†…å­˜å¤§å°ï¼Œåˆ™æ–°åˆ†é…éƒ¨åˆ†ä¸ä¼šè¢«åˆå§‹åŒ–ï¼›å¦‚æœæ–°çš„å¤§å°å°äºåŸå†…å­˜å¤§å°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±ã€‚
        // æ³¨æ„äº‹é¡¹: é‡åˆ†é…æˆåŠŸæ—§å†…å­˜ä¼šè¢«è‡ªåŠ¨é‡Šæ”¾ï¼Œæ—§æŒ‡é’ˆå˜æˆäº†é‡æŒ‡é’ˆã€‚
        // è¿”å›å€¼: å¦‚æœé‡æ–°åˆ†é…æˆåŠŸåˆ™è¿”å›æŒ‡å‘è¢«åˆ†é…å†…å­˜çš„æŒ‡é’ˆï¼Œå¦åˆ™è¿”å›ç©ºæŒ‡é’ˆ NULLã€‚
        
        // å…ˆåˆ¤æ–­å½“å‰çš„æŒ‡é’ˆæ˜¯å¦æœ‰è¶³å¤Ÿçš„è¿ç»­ç©ºé—´ï¼Œå¦‚æœæœ‰ï¼Œæ‰©å¤§ mem_address æŒ‡å‘çš„åœ°å€ï¼Œå¹¶ä¸”å°† mem_address è¿”å›ï¼Œ
        // å¦‚æœç©ºé—´ä¸å¤Ÿï¼Œå…ˆæŒ‰ç…§ newsize æŒ‡å®šçš„å¤§å°åˆ†é…ç©ºé—´ï¼Œå°†åŸæœ‰æ•°æ®ä»å¤´åˆ°å°¾æ‹·è´åˆ°æ–°åˆ†é…çš„å†…å­˜åŒºåŸŸï¼Œ
        // è€Œåé‡Šæ”¾åŸæ¥ mem_address æ‰€æŒ‡å†…å­˜åŒºåŸŸï¼ˆæ³¨æ„ï¼šåŸæ¥æŒ‡é’ˆæ˜¯è‡ªåŠ¨é‡Šæ”¾ï¼Œä¸éœ€è¦ä½¿ç”¨ freeï¼‰ï¼Œ
        // åŒæ—¶è¿”å›æ–°åˆ†é…çš„å†…å­˜åŒºåŸŸçš„é¦–åœ°å€ï¼Œå³é‡æ–°åˆ†é…å­˜å‚¨å™¨å—çš„åœ°å€ã€‚
        
        // å¯¹ emptyBucketsList è¿›è¡Œæ‰©å®¹
        emptyBucketsList = (bucket_t**)
            realloc(emptyBucketsList, newListCount * sizeof(bucket_t *));
            
        // Share newBuckets for every un-allocated size smaller than index.
        // å¯¹äºæ¯ä¸ªå°äºç´¢å¼•çš„æœªåˆ†é…å¤§å°ï¼ŒShare newBucketã€‚
        // The array is therefore always fully populated.
        // å› æ­¤ï¼Œarray å§‹ç»ˆæ€»æ˜¯å®Œå…¨å¡«å……ã€‚
        
        // æŠŠæ–°æ‰©å®¹çš„ emptyBucketsList çš„æ–°ä½ç½®ä¸Šéƒ½æ”¾ä¸Š newBuckets
        for (mask_t i = emptyBucketsListCount; i < newListCount; i++) {
            // æŠŠæ–°æ‰©å®¹çš„ emptyBucketsList çš„ æ–°ä½ç½®ä¸Šéƒ½æ”¾ä¸Š newBuckets
            emptyBucketsList[i] = newBuckets;
        }
        
        // æ›´æ–° emptyBucketsListCountï¼Œä¸” emptyBucketsListCount æ˜¯å‡½æ•°å†…çš„é™æ€å±€éƒ¨å˜é‡ï¼Œ
        // å‡½æ•°è¿›æ¥ emptyBucketsListCount éƒ½ä¿æŒä¸Šæ¬¡çš„å€¼
        emptyBucketsListCount = newListCount;
        
        // OPTION( PrintCaches, OBJC_PRINT_CACHE_SETUP, "log processing of method caches")
        if (PrintCaches) {
            // å¦‚æœå¼€å¯äº† OBJC_PRINT_CACHE_SETUP åˆ™æ‰“å° 
            _objc_inform("CACHES: new empty buckets at %p (capacity %zu)", 
                         newBuckets, (size_t)capacity);
        }
    }

    return emptyBucketsList[index];
}
```
#### CONFIG_USE_CACHE_LOCK
&emsp;`emptyBucketsForCapacity` å‡½æ•°å®ç°ç¬¬ä¸€è¡Œå°±æ˜¯ä¸€ä¸ª `CONFIG_USE_CACHE_LOCK` å®å®šä¹‰ï¼Œå®ƒæ˜¯ç”¨æ¥æ ‡å¿— `emptyBucketsForCapacity` å‡½æ•°ä½¿ç”¨ `cacheUpdateLock` è¿˜æ˜¯ `runtimeLock`ï¼Œæ³¨æ„è¿™é‡Œé’ˆå¯¹çš„æ˜¯ `Objective-C` çš„ç‰ˆæœ¬ï¼Œ`__OBJC2__` ä¸‹ä½¿ç”¨çš„æ˜¯ `runtimeLock` å¦åˆ™ä½¿ç”¨ `cacheUpdateLock`ã€‚
```c++
// OBJC_INSTRUMENTED controls whether message dispatching is dynamically monitored.
// OBJC_INSTRUMENTED æ§åˆ¶æ˜¯å¦åŠ¨æ€ç›‘è§†æ¶ˆæ¯è°ƒåº¦ã€‚

// Monitoring introduces substantial overhead.
// ç›‘æ§ä¼šå¸¦æ¥å¤§é‡å¼€é”€ã€‚

// NOTE: To define this condition, do so in the build command, NOT by uncommenting the line here.  
// NOTE: è‹¥è¦å®šä¹‰æ­¤æ¡ä»¶ï¼Œè¯·åœ¨ build å‘½ä»¤ä¸­æ‰§è¡Œæ­¤æ“ä½œï¼Œè€Œä¸æ˜¯å–æ¶ˆä¸‹é¢ OBJC_INSTRUMENTED çš„æ³¨é‡Šã€‚

// This is because objc-class.h heeds this condition, but objc-class.h can not 
// #include this file (objc-config.h) because objc-class.h is public and objc-config.h is not.
// è¿™æ˜¯å› ä¸º objc-class.h æ³¨æ„åˆ°äº†è¿™ä¸ªæ¡ä»¶ï¼Œä½†æ˜¯ objc-class.h ä¸èƒ½åŒ…æ‹¬è¿™ä¸ªæ–‡ä»¶ï¼ˆobjc-config.hï¼‰ï¼Œ
// å› ä¸º objc-class.h æ˜¯å…¬å…±çš„ï¼Œè€Œ objc-config.h ä¸æ˜¯ã€‚

//#define OBJC_INSTRUMENTED

// --------------- ä»¥ä¸Šä¸ CONFIG_USE_CACHE_LOCK æ— å…³

// In __OBJC2__, the runtimeLock is a mutex always held hence the cache 
// lock is redundant and can be elided.
// åœ¨ __OBJC2__ ä¸­ï¼ŒruntimeLock æ˜¯ "å§‹ç»ˆä¿æŒ" çš„äº’æ–¥é”ï¼Œå› æ­¤ cache lock æ˜¯å¤šä½™çš„ï¼Œå¯ä»¥å¿½ç•¥ã€‚
// (å§‹ç»ˆä¿æŒé‚£é‡Œçš„æ„æ€æ˜¯æŒ‡ runtime Lock å§‹ç»ˆæ˜¯ä¸€ä¸ªäº’æ–¥é”å—ï¼Ÿ)

// If the runtime lock ever becomes a rwlock again, the cache lock would need to be used again.
// å¦‚æœ runtime lock å†æ¬¡å˜ä¸º rwlockï¼Œåˆ™éœ€è¦å†æ¬¡ä½¿ç”¨ cache lockã€‚

// å¯åœ¨ objc-runtime-new.mm ä¸­çœ‹åˆ°å¦‚ä¸‹å®šä¹‰ï¼Œè¡¨æ˜ cacheUpdateLock åªæ˜¯ä¸€ä¸ªåœ¨æ—§ç‰ˆæœ¬ä¸­ä½¿ç”¨çš„é”
// #if CONFIG_USE_CACHE_LOCK
// mutex_t cacheUpdateLock;
// #endif

#if __OBJC2__

#define CONFIG_USE_CACHE_LOCK 0 // Objective-C 2.0 ä¸‹æ˜¯ 0 

#else

#define CONFIG_USE_CACHE_LOCK 1 //  å…¶ä»–æƒ…å†µä¸‹æ˜¯ 1

#endif

// å¯¹åº”ä¸å¦‚ä¸‹è°ƒç”¨ï¼š
#if CONFIG_USE_CACHE_LOCK
    cacheUpdateLock.assertLocked();
#else
    // extern mutex_t runtimeLock; åœ¨ objc-locks-new.h ä¸­ä¸€ä¸ªå¤–è”å£°æ˜
    // mutex_t runtimeLock; åœ¨ objc-runtime-new.mm Line 75 å®šä¹‰ï¼ˆç±»å‹æ˜¯äº’æ–¥é”ï¼‰ 
    
    // assertLocked è¿›è¡ŒåŠ é”ï¼Œå¦‚æœåŠ é”å¤±è´¥æ˜¯å¯¼è‡´æ–­è¨€
    runtimeLock.assertLocked(); // åœ¨ __OBJC2__ ä¸‹æ˜¯ä½¿ç”¨ runtimeLock
#endif
```
#### bytesForCapacity
&emsp;`bucket_t` æ•£åˆ—æ•°ç»„çš„æ€»çš„å†…å­˜å ç”¨ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚
```c++
size_t cache_t::bytesForCapacity(uint32_t cap)
{   
    // æ€»å®¹é‡ä¹˜ä»¥æ¯ä¸ª bucket_t çš„å­—èŠ‚å¤§å°
    return sizeof(bucket_t) * cap;
}
```
#### EMPTY_BYTES
```c++
// EMPTY_BYTES includes space for a cache end marker bucket.
// EMPTY_BYTES æ˜¯åŒ…æ‹¬ ç¼“å­˜ç»“æŸæ ‡è®° bucket çš„ç©ºé—´ã€‚

// This end marker doesn't actually have the wrap-around pointer 
// because cache scans always find an empty bucket before they might wrap.
// è¿™ä¸ªç»“æŸæ ‡è®°å®é™…ä¸Šæ²¡æœ‰ wrap-around æŒ‡é’ˆï¼Œ
// å› ä¸ºç¼“å­˜æ‰«ææ€»æ˜¯åœ¨å¯èƒ½è¿›è¡Œæ¢è¡Œä¹‹å‰æ‰¾åˆ°ä¸€ä¸ªç©ºçš„ bucketã€‚(å› ä¸º buckets çš„æ‰©å®¹æœºåˆ¶)

// 1024 buckets is fairly common.
// 1024 bukcets å¾ˆå¸¸è§ã€‚

// ä¸€ä¸ª bucket_t çš„å®ä¾‹å˜é‡çš„å¤§å°åº”è¯¥æ˜¯ 16 å­—èŠ‚ã€‚

#if DEBUG
    // Use a smaller size to exercise heap-allocated empty caches.
    // ä½¿ç”¨è¾ƒå°çš„å®¹é‡æ¥æ‰§è¡Œå †åˆ†é…çš„ç©ºç¼“å­˜ã€‚
#   define EMPTY_BYTES ((8+1)*16)
#else
#   define EMPTY_BYTES ((1024+1)*16)
#endif
```
### getBit/setBit/clearBit
&emsp;é’ˆå¯¹ `__LP64__` å¹³å°ä¸‹çš„ `_flags` çš„æ“ä½œã€‚[atomic_fetch_or/atomic_fetch_and](https://en.cppreference.com/w/cpp/atomic/atomic_fetch_or)
```c++
#if __LP64__
    bool getBit(uint16_t flags) const {
        return _flags & flags;
    }
    void setBit(uint16_t set) {
        __c11_atomic_fetch_or((_Atomic(uint16_t) *)&_flags, set, __ATOMIC_RELAXED);
    }
    void clearBit(uint16_t clear) {
        __c11_atomic_fetch_and((_Atomic(uint16_t) *)&_flags, ~clear, __ATOMIC_RELAXED);
    }
#endif
```
### FAST_CACHE_ALLOC_MASK
```c++
// Fast Alloc fields:
// This stores the word-aligned size of instances + "ALLOC_DELTA16", 
// or 0 if the instance size doesn't fit.
// å®ƒå­˜å‚¨å®ä¾‹çš„å­—å¯¹é½å¤§å° + "ALLOC_DELTA16"ï¼Œå¦‚æœå®ä¾‹å¤§å°ä¸é€‚åˆï¼Œåˆ™å­˜å‚¨ 0ã€‚

// These bits occupy the same bits than in the instance size, 
// so that the size can be extracted with a simple mask operation.
// è¿™äº›ä½å ç”¨ä¸å®ä¾‹å¤§å°ç›¸åŒçš„ä½ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ç®€å•çš„æ©ç æ“ä½œæå–å¤§å°ã€‚

// FAST_CACHE_ALLOC_MASK16 allows to extract the instance size rounded
// rounded up to the next 16 byte boundary, which is a fastpath for _objc_rootAllocWithZone()
// FAST_CACHE_ALLOC_MASK16 å…è®¸æå–å››èˆäº”å…¥åˆ°ä¸‹ä¸€ä¸ª 16 å­—èŠ‚è¾¹ç•Œçš„å®ä¾‹å¤§å°ï¼Œ
// è¿™æ˜¯ _objc_rootAllocWithZone() çš„å¿«é€Ÿè·¯å¾„

#define FAST_CACHE_ALLOC_MASK         0x1ff8 // 0b0001 1111 1111 1000
#define FAST_CACHE_ALLOC_MASK16       0x1ff0 // 0b0001 1111 1111 0000
#define FAST_CACHE_ALLOC_DELTA16      0x0008 // 0b0000 0000 0000 1000
```
### hasFastInstanceSize/fastInstanceSize/setFastInstanceSize
&emsp;åœ¨ `__LP64__` å¹³å°ä¸‹ï¼Œ`cache_t` å¤šäº†ä¸€ä¸ª `uint16_t _flags`ã€‚ä»¥ä¸‹å‡½æ•°æ˜¯æ ¹æ® `_flags` ä¸­çš„ä¸€äº›æ ‡å¿—ä½åšå‡ºä¸åŒçš„å¤„ç†ã€‚è¿™äº›ä¸ªå‡½æ•°å†…å®¹éƒ½æ˜¯ç»™ `objc_class` ç”¨çš„ï¼Œæœ¬ç¯‡çš„å†…å®¹æ˜¯é’ˆå¯¹çš„éƒ½æ˜¯æ–¹æ³•ç¼“å­˜çš„å­¦ä¹ ï¼Œç­‰åˆ° `objc_class` ç¯‡å†è¯¦ç»†åˆ†æä¸‹é¢çš„å‡½æ•°ã€‚
```c++
#if FAST_CACHE_ALLOC_MASK
    bool hasFastInstanceSize(size_t extra) const
    {
        if (__builtin_constant_p(extra) && extra == 0) {
            // å¦‚æœ extra åœ¨ç¼–è¯‘æ—¶æ˜¯å¸¸é‡å¹¶ä¸”å€¼ä¸º 0ï¼Œåˆ™ _flags & 0b0001 1111 1111 0000 çš„å€¼è¿”å›ï¼Œ
            // åˆ¤æ–­ cache_t æ˜¯å¦æœ‰å¿«é€Ÿå®ä¾‹åŒ–çš„å¤§å°
            return _flags & FAST_CACHE_ALLOC_MASK16;
        }
        // è¿”å› _flags & 0b0001 1111 1111 1000 çš„å€¼
        return _flags & FAST_CACHE_ALLOC_MASK;
    }

    // å¿«é€Ÿå®ä¾‹åŒ–çš„å¤§å°
    size_t fastInstanceSize(size_t extra) const
    {   
        // æ–­è¨€
        ASSERT(hasFastInstanceSize(extra));

        if (__builtin_constant_p(extra) && extra == 0) {
            // å¦‚æœ extra åœ¨ç¼–è¯‘æ—¶æ˜¯å¸¸é‡å¹¶ä¸”å€¼ä¸º 0ï¼Œåˆ™ç›´æ¥è¿”å› _flags & 0b0001 1111 1111 0000 çš„å€¼
            return _flags & FAST_CACHE_ALLOC_MASK16;
        } else {
            // size = _flags & 0b0001 1111 1111 1000
            size_t size = _flags & FAST_CACHE_ALLOC_MASK;
            
            // remove the FAST_CACHE_ALLOC_DELTA16 that was added by setFastInstanceSize
            // åˆ é™¤ç”± setFastInstanceSize æ·»åŠ çš„ FAST_CACHE_ALLOC_DELTA16
            
            // static inline size_t align16(size_t x) {
            //     return (x + size_t(15)) & ~size_t(15);
            // }
            // align16 å‡½æ•°æ˜¯è®¡ç®—å¤§äºç­‰äº x çš„æœ€å°çš„ 16 çš„å€æ•°ï¼Œå³è®¡ç®— 16 å­—èŠ‚å¯¹é½æ—¶çš„é•¿åº¦
            // 0b1000 8
            // 0b1111 15
            // 0b0001 0111  8 + 15 = 23
            // &
            // 0b0000 ~15
            // 0b0001 0000 16
            
            // 16 å­—èŠ‚å¯¹é½
            return align16(size + extra - FAST_CACHE_ALLOC_DELTA16);
        }
    }

    // è®¾ç½®å¿«é€Ÿå®ä¾‹åŒ–çš„å¤§å°
    void setFastInstanceSize(size_t newSize)
    {
        // Set during realization or construction only. No locking needed.
        // ä»…åœ¨ å®ç° æˆ– æ„é€  æœŸé—´è®¾ç½®ã€‚ä¸éœ€è¦åŠ é”ã€‚
        // #define FAST_CACHE_ALLOC_MASK         0x1ff8 // 0b0001 1111 1111 1000
        // _flags & 0b1110 0000 0000 0111
        
        uint16_t newBits = _flags & ~FAST_CACHE_ALLOC_MASK;
        uint16_t sizeBits;

        // Adding FAST_CACHE_ALLOC_DELTA16 allows for FAST_CACHE_ALLOC_MASK16 to yield
        // the proper 16byte aligned allocation size with a single mask.
        // æ·»åŠ  FAST_CACHE_ALLOC_DELTA16 å…è®¸ FAST_CACHE_ALLOC_MASK16 é€šè¿‡å•ä¸ªæ©ç äº§ç”Ÿæ­£ç¡®çš„ 16 å­—èŠ‚å¯¹é½çš„åˆ†é…å¤§å°ã€‚
        
        // #ifdef __LP64__
        // #   define WORD_MASK 7UL
        // #else
        // #   define WORD_MASK 3UL
        // #endif
        
        // 0b0101 5
        // 0b0111 7
        // 0b1100 5 + 7
        // 0b1000 ~7
        // &
        // 0b1000 // 8
        
        // static inline size_t word_align(size_t x) {
        //     return (x + WORD_MASK) & ~WORD_MASK;
        // }
        // word_align å‡½æ•°æ˜¯è¿›è¡Œå­—å¯¹é½ï¼Œå³ 8 å­—èŠ‚å¯¹é½
        
        // newSize 8 å­—èŠ‚å¯¹é½ 
        sizeBits = word_align(newSize) + FAST_CACHE_ALLOC_DELTA16;
        
        // ä¸æ“ä½œ
        sizeBits &= FAST_CACHE_ALLOC_MASK;
        
        if (newSize <= sizeBits) {
            // æˆ–æ“ä½œ
            newBits |= sizeBits;
        }
        
        // _flags èµ‹å€¼
        _flags = newBits;
    }
#else
    // ä¸æ”¯æŒ FAST_CACHE_ALLOC_MASK æ—¶ï¼Œè¿”å› false
    bool hasFastInstanceSize(size_t extra) const {
        return false;
    }
    size_t fastInstanceSize(size_t extra) const {
        // ç›´æ¥ abort
        abort();
    }
    void setFastInstanceSize(size_t extra) {
        // nothing
    }
#endif
```
#### __builtin_constant_p
&emsp;`__builtin_constant_p` æ˜¯ç¼–è¯‘å™¨ `gcc` å†…ç½®å‡½æ•°ï¼Œç”¨äºåˆ¤æ–­ä¸€ä¸ªå€¼æ˜¯å¦ä¸ºç¼–è¯‘æ—¶å¸¸é‡ï¼Œå¦‚æœæ˜¯å¸¸æ•°ï¼Œå‡½æ•°è¿”å› `1 `ï¼Œå¦åˆ™è¿”å› `0`ã€‚æ­¤å†…ç½®å‡½æ•°çš„å…¸å‹ç”¨æ³•æ˜¯åœ¨å®ä¸­ç”¨äºæ‰‹åŠ¨ç¼–è¯‘æ—¶ä¼˜åŒ–ã€‚
```c++
// åœ¨ main.m ä¸­åšå¦‚ä¸‹æµ‹è¯•ï¼š
printf("ğŸ˜ŠğŸ˜Š %d\n", __builtin_constant_p(101)); // æ‰“å°: ğŸ˜ŠğŸ˜Š 1

constexpr int a = 12 * 13;
printf("ğŸ˜ŠğŸ˜Š %d\n", __builtin_constant_p(a)); // æ‰“å°: ğŸ˜ŠğŸ˜Š 1

int a = 12 * 13;
printf("ğŸ˜ŠğŸ˜Š %d\n", __builtin_constant_p(a)); // æ‰“å°: ğŸ˜ŠğŸ˜Š 0
```
### endMarker
```c++
// CACHE_END_MARKER å€¼ä¸º 1 æ—¶ï¼Œå®šä¹‰ endMarker å‡½æ•°
bucket_t *cache_t::endMarker(struct bucket_t *b, uint32_t cap)
{
    // æœ€åä¸€ä¸ª bucket_t çš„æŒ‡é’ˆï¼Œ-1 æ˜¯ä»å†…å­˜æœ«å°¾å†å‰ç§»ä¸€ä¸ª bucket_t çš„å®½åº¦ï¼Œ
    // è¿™é‡Œæ˜¯å…ˆæŠŠ bucket_t æŒ‡é’ˆè½¬åŒ–ä¸ºä¸€ä¸ª unsigned longï¼Œç„¶ååŠ ä¸Š cap çš„å­—èŠ‚æ€»æ•°ï¼Œ
    // ç„¶åè½¬åŒ–ä¸º bucket_t æŒ‡é’ˆï¼Œç„¶åå†é€€ä¸€ä¸ªæŒ‡é’ˆçš„å®½åº¦ï¼Œå³ cache_t å“ˆå¸Œæ•°ç»„çš„æœ€åä¸€ä¸ª bucket_t çš„ä½ç½®ã€‚
    return (bucket_t *)((uintptr_t)b + bytesForCapacity(cap)) - 1;
}
```
#### CACHE_END_MARKER
&emsp;æ ‡è®°æ˜¯å¦æ”¯æŒ `cache_t` çš„ `buckets` æ•£åˆ—æ•°ç»„çš„å†…å­˜æœ«å°¾æ ‡è®°ã€‚
```c++
#if __arm__  ||  __x86_64__  ||  __i386__

// objc_msgSend has few registers available.
// objc_msgSend å¯ç”¨å¯„å­˜å™¨å¾ˆå°‘ã€‚

// Cache scan increments and wraps at special end-marking bucket.
// ç¼“å­˜æ‰«æå¢é‡åŒ…è£¹åœ¨ç‰¹æ®Šçš„æœ«ç«¯æ ‡è®°æ¡¶ä¸Šã€‚

#define CACHE_END_MARKER 1 // å®šä¸º 1

static inline mask_t cache_next(mask_t i, mask_t mask) {
    return (i+1) & mask;
}

#elif __arm64__

// objc_msgSend has lots of registers available.
// objc_msgSend æœ‰å¾ˆå¤šå¯ç”¨çš„å¯„å­˜å™¨ã€‚

// Cache scan decrements. No end marker needed.
// ç¼“å­˜æ‰«æå‡é‡ï¼Œæ— éœ€ç»“æŸæ ‡è®°ã€‚

#define CACHE_END_MARKER 0 // å®šä¸º 0

static inline mask_t cache_next(mask_t i, mask_t mask) {
    return i ? i-1 : mask;
}

#else

// æœªçŸ¥çš„æ¶æ„
#error unknown architecture

#endif
```
### reallocate
```c++
ALWAYS_INLINE
void cache_t::reallocate(mask_t oldCapacity, mask_t newCapacity, bool freeOld)
{
    // ä¸€ä¸ªä¸´æ—¶å˜é‡ç”¨äºè®°å½•æ—§çš„æ•£åˆ—è¡¨
    bucket_t *oldBuckets = buckets();
    
    // ä¸ºæ–°æ•£åˆ—è¡¨ç”³è¯·æŒ‡å®šå®¹é‡çš„ç©ºé—´
    bucket_t *newBuckets = allocateBuckets(newCapacity);

    // Cache's old contents are not propagated.
    // ç¼“å­˜çš„æ—§å†…å®¹ä¸ä¼šä¼ æ’­ã€‚
    
    // This is thought to save cache memory at the cost of extra cache fills.
    // è¿™è¢«è®¤ä¸ºæ˜¯ä»¥é¢å¤–çš„ç¼“å­˜å¡«å……ä¸ºä»£ä»·æ¥èŠ‚çœç¼“å­˜å†…å­˜çš„ã€‚
    // fixme re-measure this é‡æ–°æµ‹é‡

    ASSERT(newCapacity > 0);
    ASSERT((uintptr_t)(mask_t)(newCapacity-1) == newCapacity-1);
    
    // è®¾ç½® buckets å’Œ mask
    setBucketsAndMask(newBuckets, newCapacity - 1);
    
    if (freeOld) {
        // è¿™é‡Œä¸æ˜¯ç«‹å³é‡Šæ”¾æ—§çš„ bucketsï¼Œè€Œæ˜¯å°†æ—§çš„ buckets æ·»åŠ åˆ°å­˜æ”¾æ—§æ•£åˆ—è¡¨çš„åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç¨åé‡Šæ”¾ï¼Œæ³¨æ„è¿™é‡Œæ˜¯ç¨åé‡Šæ”¾ã€‚
        cache_collect_free(oldBuckets, oldCapacity);
    }
}
```
#### allocateBuckets
```c++
#if CACHE_END_MARKER

bucket_t *allocateBuckets(mask_t newCapacity)
{
    // Allocate one extra bucket to mark the end of the list.
    // åˆ†é…ä¸€ä¸ªé¢å¤–çš„ bucket ä»¥æ ‡è®°åˆ—è¡¨çš„æœ«å°¾ã€‚
    
    // This can't overflow mask_t because newCapacity is a power of 2.
    // å› ä¸º newCapacity æ˜¯ 2 çš„å¹‚ï¼Œæ‰€ä»¥å®ƒä¸ä¼šæº¢å‡º mask_tã€‚
    
    // ç”³è¯· sizeof(bucket_t) * newCapacity ä¸ªé•¿åº¦ä¸º 1 çš„è¿ç»­å†…å­˜ç©ºé—´ï¼Œä¸”å†…å­˜åˆå§‹åŒ–ä¸º 0
    bucket_t *newBuckets = (bucket_t *)
        calloc(cache_t::bytesForCapacity(newCapacity), 1);

    // end æ ‡è®°çš„ bucket_t
    bucket_t *end = cache_t::endMarker(newBuckets, newCapacity);

#if __arm__
    // arm 32ä½æ¶æ„ä¸‹
    // End marker's sel is 1 and imp points BEFORE the first bucket.
    // ç»“æŸæ ‡è®°çš„ sel ä¸º1ï¼Œimp æŒ‡å‘ç¬¬ä¸€ä¸ª bucket ä¹‹å‰ã€‚
    
    // This saves an instruction in objc_msgSend.
    // è¿™ä¼šå°†æŒ‡ä»¤ä¿å­˜åœ¨ objc_msgSend ä¸­ã€‚
    
    // bucket_t çš„ set å‡½æ•°ï¼Œè®¾ç½® _sel å’Œ _impï¼Œ_imp è®¾ç½®ä¸ºäº† (newBuckets - 1)
    // _sel è®¾ç½®ä¸º 1
    end->set<NotAtomic, Raw>((SEL)(uintptr_t)1, (IMP)(newBuckets - 1), nil);
#else
    // å…¶ä»–
    // End marker's sel is 1 and imp points to the first bucket.
    // ç»“æŸæ ‡è®°çš„ sel ä¸º1ï¼Œimp æŒ‡å‘ç¬¬ä¸€ä¸ªå­˜å‚¨æ¡¶ã€‚
    end->set<NotAtomic, Raw>((SEL)(uintptr_t)1, (IMP)newBuckets, nil);
#endif
    
    // ç¼“å­˜å®¹é‡ç»Ÿè®¡
    if (PrintCaches) recordNewCache(newCapacity);

    return newBuckets;
}

#else

bucket_t *allocateBuckets(mask_t newCapacity)
{
    // ç¼“å­˜å®¹é‡ç»Ÿè®¡
    if (PrintCaches) recordNewCache(newCapacity);
    
    // ç”³è¯· sizeof(bucket_t) * newCapacity ä¸ªé•¿åº¦ä¸º 1 çš„è¿ç»­å†…å­˜ç©ºé—´ï¼Œä¸”å†…å­˜åˆå§‹åŒ–ä¸º 0 
    return (bucket_t *)calloc(cache_t::bytesForCapacity(newCapacity), 1);
}

#endif
```
#### cache_collect_free
&emsp;`cache_collect_free` å‡½æ•°å£°æ˜åœ¨ `objc-cache.mm` æ–‡ä»¶é¡¶éƒ¨ï¼Œå®šä¹‰åœ¨ `objc-cache.mm` çš„ `Line 977`ã€‚
```c++
/*
cache_collect_free.

Add the specified malloc'd memory to the list of them to free at some later point.
å°†æŒ‡å®šçš„å·²åˆ†é…å†…å­˜ï¼ˆå¾…é‡Šæ”¾çš„æ–¹æ³•åˆ—è¡¨ï¼‰æ·»åŠ åˆ°å®ƒä»¬çš„åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç¨åé‡Šæ”¾ã€‚

size is used for the collection threshold. It does not have to be precisely the block's size.
size ç”¨äºæ”¶é›†é˜ˆå€¼ã€‚å®ƒä¸å¿…ç²¾ç¡®åœ°æ˜¯ å— çš„å¤§å°ã€‚

Cache locks: cacheUpdateLock must be held by the caller.
cacheUpdateLock å¿…é¡»ç”±è°ƒç”¨æ–¹æŒæœ‰ã€‚éœ€è¦åŠ é”ã€‚ï¼ˆ__objc2__ ä¸‹ä½¿ç”¨çš„æ˜¯ runtimeLockï¼‰

*/
static void cache_collect_free(bucket_t *data, mask_t capacity)
{
#if CONFIG_USE_CACHE_LOCK
    cacheUpdateLock.assertLocked();
#else
    runtimeLock.assertLocked(); // åŠ é”ï¼ŒåŠ é”å¤±è´¥æ‰§è¡Œæ–­è¨€
#endif

    // è®°å½•ç­‰å¾…é‡Šæ”¾çš„å®¹é‡ 
    if (PrintCaches) recordDeadCache(capacity);

    // ä¸º garbage å‡†å¤‡ç©ºé—´ï¼Œéœ€è¦æ—¶è¿›è¡Œæ‰©å®¹
    _garbage_make_room ();
    
    // å¢åŠ  garbage_byte_size çš„å€¼
    garbage_byte_size += cache_t::bytesForCapacity(capacity);
    
    // æŠŠæ—§çš„ buckets æ”¾è¿› garbage_refs ä¸­ï¼Œgarbage_count å¹¶è‡ªå¢ 1
    garbage_refs[garbage_count++] = data;
    
    // å°è¯•å»é‡Šæ”¾ç´¯ç§¯çš„æ—§ç¼“å­˜ï¼ˆbucket_tï¼‰
    cache_collect(false);
}
```
#### _garbage_make_room
&emsp;åŒæ · `_garbage_make_room` å‡½æ•°å£°æ˜åœ¨ `objc-cache.mm` é¡¶éƒ¨ï¼Œå®šä¹‰åœ¨ `objc-cache.mm` çš„ `Line 947`ã€‚
```c++
/*
_garbage_make_room.  
Ensure that there is enough room for at least one more ref in the garbage.
ç¡®ä¿ garbage ä¸­æœ‰è¶³å¤Ÿçš„ç©ºé—´å®¹çº³è‡³å°‘ä¸€ä¸ªå¼•ç”¨ã€‚
*/

// amount of memory represented by all refs in the garbage.
// garbage ä¸­æ‰€æœ‰å¼•ç”¨æ‰€è¡¨ç¤ºçš„å†…å­˜é‡ã€‚
static size_t garbage_byte_size = 0;

// do not empty the garbage until garbage_byte_size gets at least this big.
// åœ¨ garbage ä¸­å­—èŠ‚å¤§å°ï¼ˆgarbage_byte_sizeï¼‰è‡³å°‘è¾¾åˆ°è¿™ä¹ˆå¤§ï¼ˆgarbage_thresholdï¼‰ä¹‹å‰ï¼Œä¸è¦æ¸…ç©º garbageã€‚
static size_t garbage_threshold = 32*1024;

// table of refs to free.
// bucket_t **
static bucket_t **garbage_refs = 0;

// current number of refs in garbage_refs.
// å½“å‰ garbage_refs ä¸­ bucket_t * çš„æ•°é‡ã€‚
static size_t garbage_count = 0;

// capacity of current garbage_refs.
// å½“å‰ garbage_refs çš„å®¹é‡ã€‚
static size_t garbage_max = 0;

// capacity of initial garbage_refs
// åˆå§‹ garbage_refs çš„å®¹é‡ã€‚
enum {
    INIT_GARBAGE_COUNT = 128 // 8 ä¸ª bucket_t
};

static void _garbage_make_room(void)
{
    static int first = 1; // é™æ€å±€éƒ¨å˜é‡ï¼Œä¸‹æ¬¡è¿›æ¥ first ä¾ç„¶æ˜¯ä¸Šæ¬¡çš„å€¼

    // Create the collection table the first time it is needed
    // ç¬¬ä¸€æ¬¡éœ€è¦æ—¶åˆ›å»ºæ”¶é›†è¡¨
    if (first)
    {
        first = 0; // æ­¤å¤„ç½®ä¸º 0 åï¼Œä»¥åè°ƒç”¨ _garbage_make_room å†ä¹Ÿä¸ä¼šè¿›åˆ°è¿™ä¸ª if
        // ç”³è¯·åˆå§‹ç©ºé—´
        // ç”³è¯· INIT_GARBAGE_COUNT * sizeof(void *) å­—èŠ‚ä¸ªç©ºé—´ã€‚
        // (malloc ä¸ä¼šå¯¹ç©ºé—´è¿›è¡Œåˆå§‹åŒ–ï¼Œä¼šä¿æŒç”³è¯·æ—¶çš„åƒåœ¾æ•°æ®)
        garbage_refs = (bucket_t**)malloc(INIT_GARBAGE_COUNT * sizeof(void *));
        
        // å½“å‰ garbage_refs çš„å®¹é‡æ˜¯ INIT_GARBAGE_COUNT
        garbage_max = INIT_GARBAGE_COUNT;
    }

    // Double the table if it is full
    // å¦‚æœå½“å‰ garbage_refs ä¸­ refs çš„æ•°é‡ç­‰äº garbage_max å°±å¯¹ garbage_refs æ‰©å®¹ä¸ºå½“å‰çš„ 2 å€
    else if (garbage_count == garbage_max)
    {
        // garbage_refs æ‰©å®¹ä¸º 2 å€
        garbage_refs = (bucket_t**)
            realloc(garbage_refs, garbage_max * 2 * sizeof(void *));
        // æ›´æ–° garbage_max ä¸º 2 å€
        garbage_max *= 2;
    }
}
```
#### cache_collect
```c++
/*
cache_collect.  
Try to free accumulated dead caches.
å°è¯•é‡Šæ”¾ç´¯ç§¯çš„æ­»ç¼“å­˜ã€‚

collectALot tries harder to free memory.
collectALot å¦‚æœä¸º true åˆ™å³ä½¿ garbage_byte_size æœªè¾¾åˆ°é˜€å€¼ä¹Ÿä¼šå»é‡Šæ”¾å†…å­˜ï¼ˆæ—§çš„ bucket_tï¼‰ã€‚

Cache locks: cacheUpdateLock must be held by the caller.
cacheUpdateLock å¿…é¡»ç”±è°ƒç”¨æ–¹æŒæœ‰ï¼Œéœ€è¦åŠ é”ã€‚ï¼ˆ__objc2__ ä¸‹ä½¿ç”¨çš„æ˜¯ runtimeLockï¼‰
*/
void cache_collect(bool collectALot)
{
#if CONFIG_USE_CACHE_LOCK
    cacheUpdateLock.assertLocked();
#else
    runtimeLock.assertLocked(); // åŠ é”ï¼ŒåŠ é”å¤±è´¥ä¼šæ‰§è¡Œæ–­è¨€
#endif

    // Done if the garbage is not full
    // å¦‚æœ garbage æœªæ»¡ï¼Œåˆ™è¿”å›
    // 32 * 1024
    // æœªè¾¾åˆ°é‡Šæ”¾é˜€å€¼ï¼Œä¸” collectALot ä¸º false
    if (garbage_byte_size < garbage_threshold && !collectALot) {
        return;
    }

    // Synchronize collection with objc_msgSend and other cache readers.
    // objc_msgSend å’Œå…¶ä»– ç¼“å­˜è¯»å–å™¨ åŒæ­¥æ”¶é›†ã€‚
    if (!collectALot) {
    
        if (_collecting_in_critical ()) {
            // objc_msgSend (or other cache reader) is currently looking in the cache and might still be using some garbage.
            // objc_msgSendï¼ˆæˆ–å…¶ä»–ç¼“å­˜è¯»å–å™¨ï¼‰å½“å‰æ­£åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ï¼Œå¹¶ä¸”å¯èƒ½ä»åœ¨ä½¿ç”¨æŸäº› garbageã€‚
            // æ‰“å°
            if (PrintCaches) {
                _objc_inform ("CACHES: not collecting; "
                              "objc_msgSend in progress");
            }
            // ç›´æ¥ return
            return;
        }
    } 
    else {
        // No excuses.
        // ä¸€ç›´å¾ªç¯ç›´åˆ° _collecting_in_critical ä¸º false.
        while (_collecting_in_critical());
    }

    // No cache readers in progress - garbage is now deletable.
    // æ²¡æœ‰æ­£åœ¨è¿›è¡Œä¸­çš„ ç¼“å­˜è¯»å–å™¨ ç°åœ¨å¯ä»¥åˆ é™¤ garbage äº†ã€‚

    // Log our progress
    // Log
    if (PrintCaches) {
        cache_collections++; // è‡ªå¢
        // æ‰“å° garbage_byte_size garbage ä½¿ç”¨çš„å­—èŠ‚ cache_allocations åˆ†é…äº†å¤šå°‘ cache_t
        _objc_inform ("CACHES: COLLECTING %zu bytes (%zu allocations, %zu collections)", garbage_byte_size, cache_allocations, cache_collections);
    }
    
    // Dispose all refs now in the garbage.
    // å¤„ç† garbage ä¸­çš„æ‰€æœ‰ refsã€‚
    
    // Erase each entry so debugging tools don't see stale pointers.
    // æ“¦é™¤æ¯ä¸ªæ¡ç›®ï¼Œä»¥ä¾¿è°ƒè¯•å·¥å…·ä¸ä¼šçœ‹åˆ°è¿‡æ—¶çš„æŒ‡é’ˆã€‚
    
    // å¾ªç¯é‡Šæ”¾ garbage_refs ä¸­çš„ bucket_t *
    while (garbage_count--) {
        auto dead = garbage_refs[garbage_count];
        garbage_refs[garbage_count] = nil;
        free(dead);
    }
    
    // Clear the garbage count and total size indicator.
    // garbage_count å’Œ garbage_byte_size ç½® 0ã€‚
    
    garbage_count = 0;
    garbage_byte_size = 0;

    // æ‰“å° Cache statistics ä¸­çš„å†…å®¹
    if (PrintCaches) {
        size_t i;
        size_t total_count = 0;
        size_t total_size = 0;

        for (i = 0; i < countof(cache_counts); i++) {
            int count = cache_counts[i];
            int slots = 1 << i;
            size_t size = count * slots * sizeof(bucket_t);

            if (!count) continue;

            _objc_inform("CACHES: %4d slots: %4d caches, %6zu bytes", 
                         slots, count, size);

            total_count += count;
            total_size += size;
        }

        _objc_inform("CACHES:      total: %4zu caches, %6zu bytes", 
                     total_count, total_size);
    }
}
```
#### _collecting_in_critical
&emsp;åŒæ · `_collecting_in_critical` å‡½æ•°å£°æ˜åœ¨ `objc-cache.mm` é¡¶éƒ¨ï¼Œå®šä¹‰åœ¨ `objc-cache.mm` çš„ `Line 838`ã€‚
è¿”å› `true` è¡¨ç¤º `objc_msgSend`ï¼ˆæˆ–å…¶ä»–ç¼“å­˜è¯»å–å™¨ï¼ˆ`cache reader`ï¼‰ï¼‰å½“å‰æ­£åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ï¼Œå¹¶ä¸”å¯èƒ½ä»åœ¨ä½¿ç”¨æŸäº› `garbage`ã€‚è¿”å› `false` çš„è¯è¡¨ç¤º `garbage` ä¸­çš„ `bucket_t` æ²¡æœ‰è¢«åœ¨ä½¿ç”¨ã€‚

```c++
static int _collecting_in_critical(void)
{
#if TARGET_OS_WIN32 // å¦‚æœæ˜¯ TARGET_OS_WIN32 åˆ™ä¸€ç›´è¿”å› true
    return TRUE;
#elif HAVE_TASK_RESTARTABLE_RANGES (arm64 å’Œ x86_64 éƒ½æ”¯æŒ)
    // Only use restartable ranges if we registered them earlier.
    // å¦‚æœæˆ‘ä»¬è¾ƒæ—©æ³¨å†Œå®ƒä»¬ï¼Œè¯·ä»…ä½¿ç”¨ restartable rangesã€‚
    // #if HAVE_TASK_RESTARTABLE_RANGES
    // static bool shouldUseRestartableRanges = true;
    // #endif
    
    if (shouldUseRestartableRanges) {
        // typedef int kern_return_t;
        kern_return_t kr = task_restartable_ranges_synchronize(mach_task_self());
        
        if (kr == KERN_SUCCESS) return FALSE; // return FALSE è¡¨ç¤º garbage æ²¡æœ‰è¢«åœ¨ä½¿ç”¨ï¼Œæ­¤æ—¶å¤„äºå¯æ¸…ç©ºçŠ¶æ€ã€‚
        ï¼ˆå¦‚æœ collectALot ä¸ºçœŸåˆ™è¡¨ç¤ºå¿…é¡»æ¸…ç©ºï¼Œå¦‚æœæ˜¯ä¸å¯æ¸…ç©ºçŠ¶æ€åˆ™ä¼šä¸€ç›´ç­‰å¾…ï¼Œä¸Šé¢çš„ while (_collecting_in_critical()) ; ï¼‰
        
        _objc_fatal("task_restartable_ranges_synchronize failed (result 0x%x: %s)",
                    kr, mach_error_string(kr));
    }
#endif // !HAVE_TASK_RESTARTABLE_RANGES

    // Fallthrough if we didn't use restartable ranges.
    // å¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨ restartable rangesï¼Œåˆ™ä¼šå¤±è´¥ã€‚

    thread_act_port_array_t threads;
    unsigned number;
    unsigned count;
    kern_return_t ret;
    int result;

    // objc_thread_self: (pthread_t)tls_get_direct(_PTHREAD_TSD_SLOT_PTHREAD_SELF);
    // ä»çº¿ç¨‹çš„å­˜å‚¨ç©ºé—´è¯»å–
    
    mach_port_t mythread = pthread_mach_thread_np(objc_thread_self());

    // Get a list of all the threads in the current task.
    // è·å–å½“å‰ä»»åŠ¡ä¸­æ‰€æœ‰çº¿ç¨‹çš„åˆ—è¡¨ã€‚
#if !DEBUG_TASK_THREADS
    ret = task_threads(mach_task_self(), &threads, &number);
#else
    ret = objc_task_threads(mach_task_self(), &threads, &number);
#endif

    if (ret != KERN_SUCCESS) {
        // See DEBUG_TASK_THREADS below to help debug this.
        // è¯·å‚é˜…ä¸‹é¢çš„ DEBUG_TASK_THREADS æ¥å¸®åŠ©è°ƒè¯•ã€‚
        
        _objc_fatal("task_threads failed (result 0x%x)\n", ret);
    }

    // Check whether any thread is in the cache lookup code.
    // æ£€æŸ¥ç¼“å­˜æŸ¥æ‰¾ä»£ç ä¸­æ˜¯å¦æœ‰çº¿ç¨‹ã€‚
    
    result = FALSE;
    for (count = 0; count < number; count++)
    {
        int region;
        uintptr_t pc;

        // Don't bother checking ourselves.
        // ä¸è¦æ‰“æ‰°è‡ªå·±
        if (threads[count] == mythread)
            continue;

        // Find out where thread is executing.
        // æ‰¾å‡ºçº¿ç¨‹åœ¨å“ªé‡Œæ‰§è¡Œã€‚
        pc = _get_pc_for_thread (threads[count]);

        // Check for bad status, and if so, assume the worse (can't collect).
        // æ£€æŸ¥çŠ¶æ€æ˜¯å¦è‰¯å¥½ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å‡è®¾æƒ…å†µæ›´ç³Ÿï¼ˆæ— æ³•æ”¶é›†ï¼‰ã€‚
        if (pc == PC_SENTINEL)
        {
            result = TRUE;
            goto done;
        }
        
        // Check whether it is in the cache lookup code.
        // æ£€æŸ¥å®ƒæ˜¯å¦åœ¨ç¼“å­˜æŸ¥æ‰¾ä»£ç ä¸­ã€‚
        for (region = 0; objc_restartableRanges[region].location != 0; region++)
        {
            uint64_t loc = objc_restartableRanges[region].location;
            if ((pc > loc) &&
                (pc - loc < (uint64_t)objc_restartableRanges[region].length))
            {
                result = TRUE;
                goto done;
            }
        }
    }

 done:
    // Deallocate the port rights for the threads
    // å–æ¶ˆåˆ†é…çº¿ç¨‹çš„ç«¯å£æƒé™
    for (count = 0; count < number; count++) {
        mach_port_deallocate(mach_task_self (), threads[count]);
    }

    // Deallocate the thread list
    // å–æ¶ˆåˆ†é…çº¿ç¨‹åˆ—è¡¨
    vm_deallocate (mach_task_self (), (vm_address_t) threads, sizeof(threads[0]) * number);

    // Return our finding
    // è¿”å›æˆ‘ä»¬çš„å‘ç°
    return result;
}
```
##### HAVE_TASK_RESTARTABLE_RANGES
```c++
// Define HAVE_TASK_RESTARTABLE_RANGES to enable usage of task_restartable_ranges_synchronize().
// å®šä¹‰ HAVE_TASK_RESTARTABLE_RANGES ä»¥å¯ç”¨ä½¿ç”¨ task_restartable_ranges_synchronize() å‡½æ•°ã€‚

#if TARGET_OS_SIMULATOR || defined(__i386__) || defined(__arm__) || !TARGET_OS_MAC
#   define HAVE_TASK_RESTARTABLE_RANGES 0
#else
#   define HAVE_TASK_RESTARTABLE_RANGES 1
#endif
```
##### task_restartable_ranges_synchronize
```c++
/*!
 * @function task_restartable_ranges_synchronize
 *
 * @brief
 * Require for all threads in the task to reset their PC if within a restartable range.
 * å¦‚æœåœ¨å¯é‡æ–°å¯åŠ¨çš„èŒƒå›´å†…ï¼Œåˆ™è¦æ±‚ä»»åŠ¡ä¸­çš„æ‰€æœ‰çº¿ç¨‹é‡ç½®å…¶ PCã€‚
 *
 * @param task
 * The task to operate on (needs to be current task)
 * è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆéœ€è¦æ˜¯å½“å‰ä»»åŠ¡ï¼‰
 * @returns
 * - KERN_SUCCESS
 * - KERN_FAILURE if the task isn't the current one å¦‚æœä»»åŠ¡ä¸æ˜¯å½“å‰ä»»åŠ¡
 */
extern kern_return_t task_restartable_ranges_synchronize(task_t task);
```
&emsp;`cache_t` çš„å†…å®¹å…ˆåˆ†æåˆ°è¿™é‡Œï¼Œä¸‹ç¯‡æˆ‘ä»¬æ¥ç€æ¥çœ‹ `cache_t` å‰©ä½™çš„ `insert` å’Œ `bad_cache`ï¼Œä»¥åŠæœ€é‡è¦çš„ `objc-cache.h` æ–‡ä»¶ä¸­å£°æ˜çš„ä¸€ç³»åˆ—æ–¹æ³•ï¼Œæ­£æ˜¯å®ƒä»¬å®Œæ•´æ„æˆäº†æ–¹æ³•ç¼“å­˜çš„å®ç°ã€‚ 

## å‚è€ƒé“¾æ¥
**å‚è€ƒé“¾æ¥:ğŸ”—**
+ [æ–¹æ³•æŸ¥æ‰¾æµç¨‹ objc_msg_arm64.s](https://www.jianshu.com/p/a8668b81c5d6)
+ [OC åº•å±‚æ¢ç´¢ 09ã€objc_msgSend æµç¨‹ 1-ç¼“å­˜æŸ¥æ‰¾](https://www.cnblogs.com/zhangzhang-y/p/13704597.html)
+ [æ±‡ç¼–æŒ‡ä»¤è§£è¯»](https://blog.csdn.net/peeno/article/details/53068412)
